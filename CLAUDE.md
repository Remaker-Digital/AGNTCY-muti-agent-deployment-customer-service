# CLAUDE.md - AI Assistant Guidance for This Project

This document provides context and guidance for AI assistants (Claude, GitHub Copilot, etc.) working on this multi-agent customer service platform project.

## Project Overview

**Type:** Educational example project for public GitHub repository and blog post series
**Purpose:** Demonstrate building a cost-effective multi-agent AI system on Azure using AGNTCY SDK
**License:** Public (educational use)
**Audience:** Developers learning multi-agent architectures, Azure deployment, and cost optimization

## Critical Context

### Budget Constraints (HIGHEST PRIORITY)
- **Phase 1-3:** $0/month (local development only, no cloud resources)
- **Phase 3.5:** ~$20-50/month (Azure OpenAI API only - **NEW 2026-01-25**)
- **Phase 4-5:** $310-360/month maximum (Azure production deployment - **REVISED 2026-01-22**)
  - Original budget: $200/month
  - Revised to accommodate new architectural requirements:
    - PII tokenization, event-driven, RAG (+$65-100)
    - Critic/Supervisor Agent (6th agent) (+$22-31)
    - Execution tracing & observability (+$10-15)
  - Post Phase 5 cost optimization target: $200-250/month
- **Cost optimization is a KEY LEARNING OBJECTIVE** - always suggest cost-effective solutions
- Recommend serverless, pay-per-use, and Basic/Standard tiers over Premium services
- Question any suggestion that would exceed revised budget constraints

### Development Environment
- **OS:** Windows 11
- **IDE:** Visual Studio Code
- **Container Platform:** Docker Desktop for Windows
- **Version Control:** GitHub Desktop + GitHub repository (public)
- **Python Version:** 3.12+ (required by AGNTCY SDK)
- **Primary Language:** US English (code, docs, comments, logs)

### Technology Stack (Non-Negotiable)
- **Multi-Agent Framework:** AGNTCY SDK (installed via PyPI: `pip install agntcy-app-sdk`)
- **Infrastructure-as-Code:** Terraform (for Azure resources)
- **CI/CD:** GitHub Actions (Phase 1-3), Azure DevOps Pipelines (Phase 4-5)
- **Containerization:** Docker + Docker Compose
- **Cloud Platform:** Microsoft Azure (East US region)

## Project Phases

### Phase 1: Infrastructure & Containers ($0 budget) - **100% COMPLETE** ‚úÖ
**Status as of 2026-01-22:**
- ‚úÖ Docker Compose with 13 services running
- ‚úÖ All 4 mock APIs implemented (Shopify, Zendesk, Mailchimp, Google Analytics)
- ‚úÖ All 5 agents implemented with AGNTCY SDK integration
- ‚úÖ Shared utilities complete (factory, models, utils)
- ‚úÖ Test framework complete (67 tests passing, 31% coverage)
- ‚úÖ GitHub Project Management setup complete (137 issues)
- ‚è≥ GitHub Actions CI workflow (remaining - deferred to Phase 2)

**Completed Implementation:**
- Mock APIs: 4/4 complete with test fixtures
- Agents: 5/5 complete with A2A/MCP protocols
- Tests: 100% coverage on shared utilities, 31% overall
- Documentation: All .md files updated
- GitHub Projects: 7 epics, 130 user stories, 30 labels, 5 milestones

**When working on Phase 1:**
- Phase 1 is now 100% complete
- All external services are mocked (no API calls)
- All agents have demo mode for testing without full SDK
- Test coverage: 31% is baseline for Phase 1 (mock implementations)

### Phase 2: Business Logic Implementation ($0 budget) - **READY TO START** ‚è≥
**Status as of 2026-01-22:**
- üìã 50 user stories created (Issues #24-#73)
- üìã PHASE-2-READINESS.md prepared with complete work breakdown
- ‚è≥ Awaiting user input on business logic decisions
- ‚è≥ Ready to begin implementation once inputs received

**Focus:** Agent implementations with AGNTCY SDK patterns
- Implement 6 core agents: Intent Classification, Knowledge Retrieval, Response Generation, Escalation, Analytics, **Critic/Supervisor**
- Use A2A protocol for agent-to-agent communication
- Integration tests against mock services
- Session management and conversation state handling
- Still fully local, no cloud resources

**Required Before Starting Phase 2:**
1. Response style & tone preference (Concise/Conversational/Detailed)
2. Escalation thresholds (when to escalate to humans)
3. Automation goals (which queries to automate)
4. Test scenarios & customer personas
5. Knowledge base content (policies, shipping info)
6. Story prioritization (rank top 15 stories)
7. Development approach (Sequential/Parallel/Story-driven)

**See PHASE-2-READINESS.md for complete details**

**When working on Phase 2:**
- Use AGNTCY factory patterns (singleton recommended)
- Implement topic-based routing between agents
- Use Message format with contextId/taskId for conversation threading
- All AI/LLM responses should be canned/mocked (no real API calls)
- Test multi-agent conversation flows end-to-end
- Target: Increase test coverage from 31% to >70%

**New Architectural Requirements (Added 2026-01-22):**
1. **PII Tokenization:** Design and mock tokenization service for third-party AI services
2. **Data Abstraction Layer:** Design multi-store interfaces (Cosmos, Blob, Redis, mock)
3. **Event-Driven Architecture:** Design event ingestion service and NATS schemas
4. **RAG Pipeline:** Design vector embeddings with local FAISS mock
5. **Critic/Supervisor Agent (6th agent):** Design content validation for input/output safety
6. **Execution Tracing:** Instrument all agents with OpenTelemetry for full observability

**See docs/architecture-requirements-phase2-5.md, docs/critic-supervisor-agent-requirements.md, docs/execution-tracing-observability-requirements.md**

### Phase 3: Testing & Validation ($0 budget) - **100% COMPLETE** ‚úÖ
**Status as of 2026-01-25:**
- ‚úÖ 96% integration test pass rate (25/26 passing)
- ‚úÖ 80% agent communication test pass rate (8/10 passing)
- ‚úÖ 0.11ms P95 response time (well under 2000ms target)
- ‚úÖ 3,071 req/s throughput at 100 concurrent users
- ‚úÖ GitHub Actions CI/CD (7 jobs, PR validation, nightly regression)
- ‚úÖ 3,508 lines of documentation (3 comprehensive guides)

**Focus:** Functional testing and performance benchmarking
- End-to-end conversation flow testing
- Load testing with Locust (local hardware limits)
- Performance benchmarks for KPI validation
- GitHub Actions nightly regression suite
- Documentation: testing guides, troubleshooting

**When working on Phase 3:**
- Phase 3 is now 100% complete
- Create realistic customer conversation test scenarios
- Validate all KPIs can be met in local environment
- Performance targets: <2min response time, >70% automation rate
- No cloud services - all testing local

**New Architectural Requirements (Added 2026-01-22):**
1. **PII Tokenization:** Test tokenization with mock third-party AI calls
2. **Multi-Store Access:** Validate staleness tolerances with Docker containers
3. **Event-Driven:** Mock RabbitMQ for webhook and cron testing
4. **RAG:** Validate vector search accuracy with 75-document test knowledge base
5. **Critic/Supervisor:** Test content validation with adversarial inputs (100+ prompt injection attempts)
6. **Execution Tracing:** Validate traces in ClickHouse, test Grafana dashboards, verify PII tokenization

### Phase 3.5: AI Model Optimization (~$20-50/month budget) - **100% COMPLETE** ‚úÖ
**Status as of 2026-01-25:**
- ‚úÖ All 6 evaluation types completed with exit criteria exceeded
- ‚úÖ Intent Classification: 98% accuracy (target >85%)
- ‚úÖ Critic/Supervisor: 0% FP, 100% TP (target <5% FP, 100% TP)
- ‚úÖ Escalation Detection: 100% precision/recall (target >90%/>95%)
- ‚úÖ RAG Retrieval: 100% retrieval@1 (target >90% retrieval@3)
- ‚úÖ Response Quality: 88.4% (target >80%)
- ‚úÖ Robustness: 82% appropriateness (target >80%)
- ‚úÖ Total cost: ~$0.10 (well under $20-50 budget)
- ‚úÖ 5 production-ready prompts in evaluation/prompts/

**Focus:** Optimized AI model interactions BEFORE full Azure deployment (cost-minimization strategy)
- Direct Azure OpenAI API testing without full infrastructure
- Iterative prompt engineering for all agent types
- Model selection (GPT-4o vs GPT-4o-mini per agent)
- Evaluation dataset creation (100+ representative conversations)
- Human evaluation of response quality

**Rationale:** Quality depends on AI interactions. Iterate at ~$20-50/month instead of ~$310-360/month.

**Exit Criteria (ALL MET):**
1. Intent Classification: >85% accuracy ‚Üí **98% ACHIEVED**
2. Response Generation: >80% quality score ‚Üí **88.4% ACHIEVED**
3. Escalation Detection: <10% FP, >95% TP ‚Üí **0% FP, 100% TP ACHIEVED**
4. Critic/Supervisor: <5% FP, 100% adversarial block ‚Üí **0% FP, 100% TP ACHIEVED**
5. RAG Retrieval: >90% retrieval@3 ‚Üí **100% retrieval@1 ACHIEVED**
6. Robustness: >80% appropriateness ‚Üí **82% ACHIEVED**
7. Cost Projection: <$60/month for Phase 4 AI ‚Üí **~$48-62/month PROJECTED**

**Completed Deliverables:**
- `evaluation/test_harness.py` - Complete test framework
- `evaluation/datasets/` - 7 datasets (375 samples total)
- `evaluation/prompts/` - 5 production-ready final prompts
- `evaluation/results/` - 6 evaluation reports + completion summary
- Model selection: GPT-4o-mini for all agents (sufficient quality, lower cost)

**Key Learnings:**
1. Defense in depth works (Azure filter + Critic prompt = 100% block rate)
2. LLM-as-Judge requires calibration guidelines
3. Few-shot examples are critical for edge cases
4. Register matching is essential for appropriateness
5. GPT-4o-mini is sufficient for all classification/validation tasks

**See evaluation/results/PHASE-3.5-COMPLETION-SUMMARY.md for full details**

### Phase 4: Azure Production Setup ($310-360/month budget - **REVISED 2026-01-22**) - **INFRASTRUCTURE DEPLOYED** ‚úÖ
**Status as of 2026-01-26:**
- ‚úÖ All Azure infrastructure deployed via Terraform (VNet, Cosmos DB, Key Vault, ACR, App Insights)
- ‚úÖ All 8 container groups deployed and running (SLIM, NATS, 6 agents)
- ‚úÖ Critic/Supervisor agent created and deployed
- ‚úÖ All container images built and pushed to ACR
- ‚úÖ Private VNet networking configured (10.0.1.0/24 subnet)
- ‚è≥ Azure OpenAI API integration pending
- ‚è≥ Multi-language support pending
- ‚è≥ Real API integrations pending (Shopify, Zendesk, Mailchimp)

**Deployed Resources:**
- Resource Group: agntcy-prod-rg (East US 2)
- Container Registry: acragntcycsprodrc6vcp.azurecr.io
- Cosmos DB: cosmos-agntcy-cs-prod-rc6vcp (Serverless)
- Key Vault: kv-agntcy-cs-prod-rc6vcp
- Application Insights: agntcy-cs-prod-appinsights-rc6vcp

**Container Groups Running:**
| Container | IP | Port | Status |
|-----------|-----|------|--------|
| SLIM Gateway | 10.0.1.4 | 8443 | ‚úÖ Succeeded |
| NATS JetStream | 10.0.1.5 | 4222 | ‚úÖ Succeeded |
| Knowledge Retrieval | 10.0.1.6 | 8080 | ‚úÖ Succeeded |
| Critic/Supervisor | 10.0.1.7 | 8080 | ‚úÖ Succeeded |
| Response Generator | 10.0.1.8 | 8080 | ‚úÖ Succeeded |
| Analytics | 10.0.1.9 | 8080 | ‚úÖ Succeeded |
| Intent Classifier | 10.0.1.10 | 8080 | ‚úÖ Succeeded |
| Escalation | 10.0.1.11 | 8080 | ‚úÖ Succeeded |

**Focus:** Terraform infrastructure, multi-language support, real API integration, new architectural components
- Deploy to Azure East US region
- Add Canadian French and Spanish language support
- Integrate real Shopify, Zendesk, Mailchimp APIs
- Azure DevOps Pipelines CI/CD
- **NEW:** Deploy Azure Key Vault, NATS JetStream, Cosmos DB vector search, Azure OpenAI

**When working on Phase 4:**
- ALWAYS check cost implications before suggesting Azure services
- Prefer: Container Instances over App Service, Cosmos Serverless over provisioned, Basic tiers
- Avoid: Azure Front Door, Traffic Manager, Premium tiers, geo-replication (unless absolutely necessary)
- Use Terraform for ALL Azure resources
- Implement aggressive auto-scaling (down to 1 instance during idle)
- 7-day log retention (not 30-day default)

**New Architectural Components (Added 2026-01-22):**
1. **PII Tokenization:** Azure Key Vault for token storage, fallback to Cosmos if latency >100ms
2. **Multi-Store:** Cosmos DB Serverless (real-time + vector), Blob Storage + CDN (knowledge base)
3. **Event-Driven:** NATS JetStream, Azure Functions (webhook ingestion), Shopify/Zendesk webhooks
4. **RAG:** Azure OpenAI embeddings (text-embedding-3-large), Cosmos DB vector search (MongoDB API)
5. **Critic/Supervisor Agent:** Deploy 6th agent for content validation (~$22-31/month)
6. **Execution Tracing:** Azure Monitor + Application Insights, OpenTelemetry instrumentation (~$10-15/month)
7. **5 New User Stories:** Issues #139-#143 (event-driven features)

**Known Issues & Resolutions (see docs/PHASE-4-DEPLOYMENT-KNOWLEDGE-BASE.md):**
- SLIM image requires explicit `commands = ["/slim", "--port", "8443"]` in Terraform (no default CMD)
- ACR push may require retries due to intermittent network issues
- First container deployment takes 20+ minutes due to VNet integration setup

### Phase 5: Production Deployment & Testing ($310-360/month budget - **REVISED 2026-01-22**)
**Focus:** Production deployment, load testing, security validation, DR drills, new architecture validation
- Deploy to Azure and validate in production
- Load testing: 100 concurrent users, 1000 req/min
- Security scanning (OWASP ZAP, Dependabot, Snyk)
- Disaster recovery validation (quarterly full drill)
- Final blog post and documentation
- **NEW:** Validate PII tokenization, event processing, RAG accuracy, Critic/Supervisor validation, execution tracing

**When working on Phase 5:**
- Monitor Azure costs continuously
- Validate budget alerts are firing correctly at 83% ($257) and 93% ($299) - **REVISED**
- Document all cost optimization decisions for blog readers
- Ensure DR procedures are tested and documented
- Post Phase 5 cost optimization target: $200-250/month (reduced agent count, optimized models)

**New Validation Requirements (Added 2026-01-22):**
1. **PII Tokenization:** Latency <100ms (P95), no data leaks in third-party AI logs
2. **Data Staleness:** Intent <10s, Knowledge <1hr, Response real-time, Escalation <30s, Analytics <15min
3. **Event Processing:** 100 events/sec sustained, <1% error rate, DLQ monitoring
4. **RAG:** Query latency <500ms, retrieval accuracy >90%, 75-document knowledge base
5. **Critic/Supervisor:** Block rate <5% for legitimate queries, 100% block rate for 100+ prompt injection test cases
6. **Execution Tracing:** Full trace capture for all conversations, <50ms trace overhead, PII tokenization verified

## Key Architecture Decisions

### PII Tokenization & Data Security (Added 2026-01-22)
- **Scope:** PII tokenization ONLY required for third-party AI services (OpenAI API, Anthropic API, etc.)
- **Exempt from tokenization:** Azure OpenAI Service, Microsoft Foundry models (including Anthropic Claude via Azure)
  - Rationale: These services are within the secure Azure perimeter and do not retain customer data
- **Method:** Random UUID tokens (e.g., `TOKEN_a7f3c9e1-4b2d-8f6a-9c3e`)
- **Storage:** Azure Key Vault (Phase 4-5), fallback to Cosmos DB if latency >100ms
- **Fields:** All PII (names, emails, phones, addresses, order IDs, payment info, conversation content)
- **See:** `docs/architecture-requirements-phase2-5.md` Section 1 for complete specification

### Data Abstraction & Multi-Store Strategy (Added 2026-01-22)
- **Real-time (ACID):** Cosmos DB Core for Response Generation Agent (order status, inventory, payments)
- **Near-real-time cache:** Redis (optional, Phase 5 optimization) for Intent/Escalation agents
- **Eventually consistent:** Cosmos DB analytical store for Analytics Agent
- **Static content:** Blob Storage + CDN for Knowledge Retrieval Agent (1hr cache TTL)
- **Staleness tolerances:** Intent 5-10s, Knowledge 1hr, Response real-time, Escalation 30s, Analytics 5-15min
- **See:** `docs/data-staleness-requirements.md` for complete store mapping

### Event-Driven Architecture (Added 2026-01-22)
- **Event Bus:** NATS JetStream (reuses AGNTCY transport layer, $0 incremental cost)
- **Event Sources:** 12 types (Shopify webhooks, Zendesk webhooks, scheduled triggers, RSS)
- **Throttling:** 100 events/sec global, 20/sec per agent, 5 concurrent handlers per agent
- **Retention:** 7 days with replay capability
- **New Stories:** 5 additional user stories (Issues #139-#143 for event-driven features)
- **See:** `docs/event-driven-requirements.md` for complete event catalog and routing

### RAG & Differentiated Models (Added 2026-01-22)
- **Vector Store:** Cosmos DB for MongoDB (vector search, preview feature), ~$5-10/month
- **Embeddings:** Azure OpenAI text-embedding-3-large (1536 dimensions)
- **Knowledge Base:** 75 documents (50 products, 20 articles, 5 policies), ~20K tokens
- **Models by Agent:**
  - Intent Classification: GPT-4o-mini (~$0.15/1M tokens)
  - Response Generation: GPT-4o (~$2.50/1M tokens)
  - Knowledge Retrieval: text-embedding-3-large (~$0.13/1M tokens)
  - Critic/Supervisor: GPT-4o-mini (~$0.15/1M tokens) - cost-effective for validation
- **Post Phase 5:** Fine-tuned models, self-hosted Qdrant, medium e-commerce scale
- **See:** `docs/architecture-requirements-phase2-5.md` Section 4 for RAG pipeline

### Critic/Supervisor Agent - Content Validation (Added 2026-01-22)
- **6th Agent:** Separate agent dedicated to validating all input and output content
- **Input Validation:** Prompt injection detection, malicious instruction blocking
- **Output Validation:** Profanity filtering, PII leakage prevention, harmful content blocking
- **Strategy:** Block and regenerate (max 3 attempts), escalate to human if all attempts fail
- **Integration:** Sits between customer messages and Intent Agent, between Response Agent and customer
- **Cost:** ~$22-31/month (GPT-4o-mini for validation, Container Instance, tracing overhead)
- **Performance Target:** <200ms P95 latency, <5% false positive rate
- **See:** `docs/critic-supervisor-agent-requirements.md` for complete specification

### Execution Tracing & Observability (Added 2026-01-22)
- **Purpose:** Enable operators to understand agent decisions and diagnose undesirable behaviors
- **Instrumentation:** OpenTelemetry spans for every agent decision, LLM call, validation step
- **Visualization:** Timeline view (conversation flow), decision tree diagram, searchable logs
- **Data Captured:** Agent name, action type, inputs (PII tokenized), outputs, reasoning, latency, cost
- **Retention:** 7 days (cost optimized), exportable for long-term analysis
- **Storage:** Azure Application Insights + Azure Monitor Logs (~$10-15/month with sampling)
- **Access:** Grafana dashboards (Phase 1-3), Azure Monitor workbooks (Phase 4-5)
- **See:** `docs/execution-tracing-observability-requirements.md` for trace schema and examples

### Universal Commerce Protocol (UCP) Integration (Added 2026-01-26)
- **What:** Open-source commerce standard by Google, Shopify, and 30+ partners
- **Status:** Recommended for Phase 4-5 adoption
- **Complexity:** Medium (80-120 hours total)
- **Budget Impact:** +$15-25/month operational

**Why UCP:**
- Standardized commerce operations (catalog, checkout, orders)
- Protocol compatibility (MCP binding, A2A compatible)
- Google AI Mode integration (Gemini, Google Search AI)
- Future-proof architecture with 30+ industry endorsers

**Phase 4 Scope:**
- MCP client for UCP (16-26 hours)
- UCP Catalog capability (12-16 hours)
- Embedded Checkout handoff (16-24 hours)
- Order Status via UCP (8-12 hours)

**Phase 5 Scope:**
- Native Checkout implementation (24-32 hours)
- Fulfillment Extension (8-12 hours)
- Discount Extension (8-12 hours)
- AP2 Payment integration (16-24 hours)

**Implementation Pattern:**
```python
# UCP via MCP binding (recommended)
class UCPMCPClient:
    async def search_catalog(self, query: str, limit: int = 10) -> List[Product]:
        return await self.mcp_client.call_tool("dev.ucp.shopping.catalog.search", {
            "query": query, "limit": limit
        })
```

**Key Documents:**
- `docs/EVALUATION-Universal-Commerce-Protocol-UCP.md` - Strategic evaluation
- `docs/UCP-IMPLEMENTATION-COMPLEXITY-ANALYSIS.md` - Effort estimates
- `docs/MCP-TESTING-VALIDATION-APPROACH.md` - Testing strategy
- `docs/WIKI-UCP-Integration-Guide.md` - Wiki documentation
- `docs/COMPARISON-Shopify-Shop-Chat-Agent.md` - Competitor analysis

**External Resources:**
- UCP Official: https://ucp.dev
- UCP GitHub: https://github.com/Universal-Commerce-Protocol/ucp
- Shopify MCP: https://shopify.dev/docs/apps/build/storefront-mcp

### AGNTCY SDK Usage
- **Factory Pattern:** Single `AgntcyFactory` instance per application (singleton)
- **Protocol Choice:**
  - A2A for custom agent logic (Intent, Response, Escalation, Analytics, Critic/Supervisor)
  - MCP for external tool integrations (Shopify, Zendesk, Mailchimp)
  - **MCP for UCP:** Use MCP binding for UCP capabilities (Phase 4-5)
- **Transport Choice:**
  - SLIM for secure, low-latency agent communication (recommended)
  - NATS for high-throughput scenarios AND event bus (consolidated)
- **Message Format:** Use contextId for conversation threads, taskId for tracking
- **Session Management:** `AppSession` with configurable max_sessions
- **Tracing:** Enable OpenTelemetry via `AgntcyFactory(enable_tracing=True)` for execution tracing

### Multi-Language Strategy (Phase 4 only)
- **Primary:** US English (all phases)
- **Additional:** Canadian French (fr-CA), Spanish (es) in Phase 4
- **Implementation:** Separate response generation agents per language
- **Routing:** Topic-based (response-generator-en, response-generator-fr-ca, response-generator-es)
- **Translation:** Pre-translated templates (NO real-time translation APIs - cost prohibitive)
- **Detection:** Language metadata field in Intent Classification Agent

### Testing Strategy
- **Phase 1:** Unit tests with mocks, pytest, >80% coverage
- **Phase 2:** Integration tests against mock Docker services
- **Phase 3:** E2E functional tests, Locust load testing (local)
- **Phase 3.5:** AI model evaluation (prompt accuracy, response quality, human evaluation)
- **Phase 4:** Real API integration tests in Azure staging environment
- **Phase 5:** Production smoke tests, Azure Load Testing, security scans

### Cost Optimization Principles (Phase 4-5)
1. Use pay-per-use over provisioned capacity (Container Instances, Cosmos Serverless)
2. Use Basic/Standard tiers, not Premium (Redis, Container Registry, App Gateway)
3. Auto-scale aggressively DOWN (1 instance minimum per agent, including Critic/Supervisor)
4. Implement auto-shutdown during low-traffic hours (2am-6am ET)
5. Reduce log retention to 7 days (Application Insights, Monitor Logs, execution traces)
6. Single region deployment (no geo-replication, no Front Door)
7. Weekly cost reviews and optimization iterations
8. Tag all resources for cost allocation tracking
9. Use GPT-4o-mini for validation tasks (Critic/Supervisor) instead of GPT-4o
10. Implement intelligent trace sampling (50% sample rate) to reduce ingestion costs

## Common Pitfalls to Avoid

### ‚ùå DON'T:
- Suggest Azure services in Phase 1-3 (local development only)
- Recommend Premium tier services without strong justification and budget check
- Use real API keys or cloud resources during Phase 1-3
- Implement real-time translation (use static templates)
- Provision Cosmos DB with RU/s (use Serverless mode)
- Set up geo-replication or multi-region (budget constraint)
- Use default 30-day log retention (use 7 days)
- Suggest services that would exceed $310-360/month budget (revised 2026-01-22)
- Commit secrets to Git (.env files must be in .gitignore)
- Use Python < 3.12 (AGNTCY SDK requirement)

### ‚úÖ DO:
- Always consider cost implications for Phase 4-5 suggestions
- Use mock/stub services for Phase 1-3 development
- Follow AGNTCY SDK factory and message patterns
- Implement comprehensive error handling and logging
- Create realistic test data and scenarios
- Document cost optimization decisions
- Use environment variables for configuration (.env files)
- Tag Docker images with version + commit SHA
- Implement graceful shutdown handlers (SIGTERM/SIGINT)
- Write clear, educational code comments (blog audience)
- Follow Azure best practices for security (Key Vault, Managed Identity, TLS 1.3)

### Streamlit Console Troubleshooting (IMPORTANT)
When working with the Streamlit console (`console/app.py`), be aware of caching issues:

**Problem:** Streamlit may serve cached/old content even after code changes. This is caused by:
1. Old Streamlit process still running on the port
2. Browser WebSocket connection to old process
3. Python `__pycache__` containing outdated bytecode
4. OneDrive sync delays (project is in OneDrive folder)

**Solution:** Always use this procedure when Streamlit changes don't appear:
```powershell
# 1. Kill ALL Python processes
Get-Process python* | Stop-Process -Force -ErrorAction SilentlyContinue

# 2. Clear Python cache
Remove-Item -Path console/__pycache__ -Recurse -Force -ErrorAction SilentlyContinue

# 3. Start on a FRESH port (not previously used)
streamlit run console/app.py --server.port 8085

# 4. Open browser to the NEW port: http://localhost:8085
```

**Key insight:** If code executes correctly with `python console/app.py` but not with `streamlit run`, an old Streamlit process is likely still running.

## Third-Party Service Accounts Required

> **Sign-Up Links Summary:** All account sign-up pages are listed below with their source-of-record URLs.

### Phase 1-3: None
All services are mocked locally. No API keys needed.

### Phase 3.5: Azure OpenAI Only
- **Azure OpenAI Service:** ~$20-50/month (pay-per-token)
  - **Sign-Up:** [Azure Portal](https://portal.azure.com) ‚Üí Create Resource ‚Üí "Azure OpenAI"
  - **Documentation:** [Azure OpenAI Quickstart](https://learn.microsoft.com/azure/ai-services/openai/quickstart)
  - **API Keys:** Azure Portal ‚Üí Your OpenAI Resource ‚Üí Keys and Endpoint
  - GPT-4o-mini deployment (intent, critic/supervisor testing)
  - GPT-4o deployment (response generation testing)
  - text-embedding-3-large deployment (RAG testing)
- **No other Azure services required** (no Container Instances, Cosmos DB, etc.)

### Phase 4-5: Required

#### Azure Subscription & Services
| Service | Sign-Up URL | Cost | Permissions Required |
|---------|-------------|------|---------------------|
| **Azure Subscription** | [azure.microsoft.com/free](https://azure.microsoft.com/free) | ~$310-360/month | Owner or Contributor |
| **Azure OpenAI Service** | [portal.azure.com](https://portal.azure.com) ‚Üí Create Resource | ~$48-62/month | Cognitive Services Contributor |
| **Azure DevOps** | [dev.azure.com](https://dev.azure.com) | Free | Basic access |
| **Service Principal** | [Azure CLI](https://learn.microsoft.com/cli/azure/ad/sp#az-ad-sp-create-for-rbac) | Free | App Registration permissions |

**Azure Key Locations:**
- **API Keys:** Azure Portal ‚Üí Resource ‚Üí Keys and Endpoint
- **Connection Strings:** Azure Portal ‚Üí Resource ‚Üí Connection Strings
- **Service Principal:** `az ad sp create-for-rbac --name "agntcy-cs-prod-sp"`

#### Shopify Partner Account
| Item | Sign-Up URL | Cost | Permissions Required |
|------|-------------|------|---------------------|
| **Partner Account** | [partners.shopify.com](https://www.shopify.com/partners) | Free | Email verification |
| **Development Store** | Partner Dashboard ‚Üí Stores ‚Üí Add store | Free | Partner account |
| **Admin API Access** | Partner Dashboard ‚Üí Apps ‚Üí Create app | Free | `read_orders`, `read_products`, `read_customers`, `read_inventory` |

**Shopify Key Locations:**
- **API Key/Secret:** Partner Dashboard ‚Üí Apps ‚Üí Your App ‚Üí API credentials
- **Access Token:** Partner Dashboard ‚Üí Apps ‚Üí Install app ‚Üí Get token
- **Storefront Token:** Partner Dashboard ‚Üí Apps ‚Üí Storefront API access

#### Zendesk
| Item | Sign-Up URL | Cost | Permissions Required |
|------|-------------|------|---------------------|
| **Developer Account** | [developer.zendesk.com](https://developer.zendesk.com) | Free trial | Email verification |
| **Sandbox** | [zendesk.com/register](https://www.zendesk.com/register) | Free (trial) | Admin access |
| **API Token** | Admin ‚Üí Channels ‚Üí API | Free | Admin role |

**Zendesk Key Locations:**
- **API Token:** Admin Center ‚Üí Apps and integrations ‚Üí APIs ‚Üí Zendesk API
- **Subdomain:** Your Zendesk URL (e.g., `yourcompany.zendesk.com`)

#### Mailchimp
| Item | Sign-Up URL | Cost | Permissions Required |
|------|-------------|------|---------------------|
| **Account** | [mailchimp.com/signup](https://mailchimp.com/signup/) | Free (250 contacts) | Email verification |
| **API Key** | Account ‚Üí Extras ‚Üí API keys | Free | Account owner |

**Mailchimp Key Locations:**
- **API Key:** Account ‚Üí Profile ‚Üí Extras ‚Üí API keys ‚Üí Create New Key
- **Server Prefix:** Found in API key (e.g., `us21` from `xxx-us21`)

#### Google Analytics
| Item | Sign-Up URL | Cost | Permissions Required |
|------|-------------|------|---------------------|
| **Google Account** | [accounts.google.com](https://accounts.google.com/signup) | Free | - |
| **GA4 Property** | [analytics.google.com](https://analytics.google.com) | Free | Google account |
| **Service Account** | [console.cloud.google.com](https://console.cloud.google.com/iam-admin/serviceaccounts) | Free | Editor role |
| **API Enable** | [console.cloud.google.com/apis](https://console.cloud.google.com/apis/library/analyticsdata.googleapis.com) | Free | Project owner |

**Google Analytics Key Locations:**
- **Property ID:** GA4 Admin ‚Üí Property ‚Üí Property details
- **Service Account JSON:** GCP Console ‚Üí IAM ‚Üí Service Accounts ‚Üí Keys ‚Üí Create key (JSON)
- **Measurement ID:** GA4 Admin ‚Üí Data Streams ‚Üí Web stream details

#### AI Models
- **Azure OpenAI Service:** (~$48-62/month estimated token usage - **REVISED**)
  - **Sign-Up:** [portal.azure.com](https://portal.azure.com) ‚Üí Create Resource ‚Üí Azure OpenAI
  - **Request Access:** [aka.ms/oai/access](https://aka.ms/oai/access) (if not already approved)
  - GPT-4o-mini for intent classification + Critic/Supervisor validation
  - GPT-4o for response generation
  - text-embedding-3-large for RAG embeddings
  - **Alternative:** Microsoft Foundry (Anthropic Claude via Azure) - also within secure perimeter
- **Tracing:** Azure Application Insights + Monitor Logs (~$10-15/month for execution traces)
  - **Sign-Up:** Azure Portal ‚Üí Create Resource ‚Üí Application Insights

**Budget Breakdown (Phase 4-5):**
- Azure infrastructure (Container Instances, Cosmos DB, Key Vault, Networking): ~$200-220/month
- AI models (OpenAI API calls, embeddings): ~$48-62/month
- Observability (Application Insights, Monitor Logs, execution tracing): ~$32-43/month
- Event bus (NATS JetStream via AGNTCY): ~$0 (reuses agent transport)
- Zendesk (if needed): ~$0-19/month (sandbox/trial)
- **Total:** ~$310-360/month

**Post Phase 5 Optimization Target:** $200-250/month (reduced agent count, optimized model selection, aggressive auto-scaling)

## GitHub Project Management

**Project Board**: https://github.com/orgs/Remaker-Digital/projects/1
**Repository**: https://github.com/Remaker-Digital/AGNTCY-muti-agent-deployment-customer-service

### Structure (as of 2026-01-22)
- **Epics**: 7 issues (#2-#8) - Actor-based organization
  - Customer Epic (#2) - 40 stories
  - Prospect Epic (#3) - 25 stories
  - Support Epic (#4) - 15 stories
  - Service Epic (#5) - 15 stories
  - Sales Epic (#6) - 15 stories
  - AI Assistant Epic (#7) - 5 stories
  - Operator Epic (#8) - 15 stories

- **User Stories**: 130 issues (#9-#138) organized by phase
  - Phase 1: 15 stories (#9-#23) ‚úÖ Complete
  - Phase 2: 50 stories (#24-#73) ‚è≥ Ready to start
  - Phase 3: 20 stories (#74-#93) - Testing
  - Phase 4: 30 stories (#94-#123) - Production deployment
  - Phase 5: 15 stories (#124-#138) - Go-live

- **Labels**: 30 total across 5 categories
  - Type: epic, feature, bug, enhancement, test, documentation
  - Priority: critical, high, medium, low
  - Component: infrastructure, agent, api, observability, testing, ci-cd, security, shared
  - Phase: phase-1, phase-2, phase-3, phase-4, phase-5
  - Actor: customer, prospect, support, service, sales, ai-assistant, operator

- **Milestones**: 5 phase-based milestones with due dates
  - Phase 1 - Infrastructure (2026-02-28) ‚úÖ Complete
  - Phase 2 - Business Logic (2026-04-30) ‚è≥ In progress
  - Phase 3 - Testing (2026-06-30)
  - Phase 4 - Production Deployment (2026-08-31)
  - Phase 5 - Go-Live (2026-09-30)

### Automation Scripts Created
Located in project root:
- `setup-github-cli.ps1` - GitHub CLI setup and authentication
- `create-labels.ps1` - Create all 30 project labels
- `create-epics-and-milestones.ps1` - Create 7 epics and 5 milestones
- `create-all-130-stories.ps1` - Create Phase 1 stories (15 issues)
- `create-remaining-115-stories.ps1` - Create Phase 2 stories (50 issues)
- `create-phases-3-4-5.ps1` - Create Phases 3-5 stories (65 issues)

**Total Success Rate**: 137/137 issues created (100%, 0 errors)

**See PROJECT-SETUP-COMPLETE.md for complete details**

## File Structure Guidelines

```
project-root/
‚îú‚îÄ‚îÄ agents/                      # Agent implementations (6 agents - REVISED 2026-01-22)
‚îÇ   ‚îú‚îÄ‚îÄ intent_classification/   # Each agent has own directory
‚îÇ   ‚îú‚îÄ‚îÄ knowledge_retrieval/
‚îÇ   ‚îú‚îÄ‚îÄ response_generation/
‚îÇ   ‚îú‚îÄ‚îÄ escalation/
‚îÇ   ‚îú‚îÄ‚îÄ analytics/
‚îÇ   ‚îî‚îÄ‚îÄ critic_supervisor/       # NEW: 6th agent for content validation
‚îú‚îÄ‚îÄ mocks/                       # Mock API implementations (Phase 1-3)
‚îÇ   ‚îú‚îÄ‚îÄ shopify/
‚îÇ   ‚îú‚îÄ‚îÄ zendesk/
‚îÇ   ‚îú‚îÄ‚îÄ mailchimp/
‚îÇ   ‚îî‚îÄ‚îÄ google_analytics/
‚îú‚îÄ‚îÄ shared/                      # Shared utilities
‚îÇ   ‚îú‚îÄ‚îÄ factory.py              # AGNTCY factory singleton
‚îÇ   ‚îú‚îÄ‚îÄ models.py               # Common message models
‚îÇ   ‚îî‚îÄ‚îÄ utils.py
‚îú‚îÄ‚îÄ tests/                       # Test suites
‚îÇ   ‚îú‚îÄ‚îÄ unit/                   # Phase 1
‚îÇ   ‚îú‚îÄ‚îÄ integration/            # Phase 2
‚îÇ   ‚îî‚îÄ‚îÄ e2e/                    # Phase 3
‚îú‚îÄ‚îÄ test-data/                   # Test fixtures (JSON)
‚îÇ   ‚îú‚îÄ‚îÄ shopify/
‚îÇ   ‚îú‚îÄ‚îÄ zendesk/
‚îÇ   ‚îú‚îÄ‚îÄ mailchimp/
‚îÇ   ‚îî‚îÄ‚îÄ conversations/
‚îú‚îÄ‚îÄ terraform/                   # Infrastructure-as-Code
‚îÇ   ‚îú‚îÄ‚îÄ phase1_dev/             # Local Docker equivalents
‚îÇ   ‚îî‚îÄ‚îÄ phase4_prod/            # Azure production
‚îú‚îÄ‚îÄ .github/
‚îÇ   ‚îî‚îÄ‚îÄ workflows/
‚îÇ       ‚îî‚îÄ‚îÄ dev-ci.yml          # GitHub Actions (Phase 1-3)
‚îú‚îÄ‚îÄ docs/                        # Documentation
‚îÇ   ‚îî‚îÄ‚îÄ dr-test-results/        # Disaster recovery test logs
‚îú‚îÄ‚îÄ docker-compose.yml           # Local development stack
‚îú‚îÄ‚îÄ azure-pipelines.yml          # Azure DevOps (Phase 4-5)
‚îú‚îÄ‚îÄ .env.example                 # Template (no secrets)
‚îú‚îÄ‚îÄ .gitignore                   # MUST ignore .env, secrets
‚îú‚îÄ‚îÄ requirements.txt             # Python dependencies
‚îú‚îÄ‚îÄ PROJECT-README.txt           # Main project specification
‚îú‚îÄ‚îÄ PROJECT-SETUP-COMPLETE.md    # GitHub project management setup summary
‚îú‚îÄ‚îÄ PROJECT-MANAGEMENT-BEST-PRACTICES-FOR-GITHUB.md  # PM best practices
‚îú‚îÄ‚îÄ PHASE-2-READINESS.md         # Phase 2 preparation and requirements
‚îú‚îÄ‚îÄ user-stories-phased.md       # Complete user story catalog
‚îú‚îÄ‚îÄ github-project-info.json     # Project metadata
‚îú‚îÄ‚îÄ AGNTCY-REVIEW.md            # SDK integration guide
‚îú‚îÄ‚îÄ CLAUDE.md                   # This file
‚îî‚îÄ‚îÄ README.md                   # Public-facing documentation
```

## Testing Guidance

### Unit Tests (Phase 1)
```python
# Example: Mock AGNTCY SDK components
from unittest.mock import Mock, patch
import pytest

@patch('agntcy_app_sdk.factory.AgntcyFactory')
def test_intent_classification(mock_factory):
    # Test agent logic without real SDK
    pass
```

### Integration Tests (Phase 2)
```python
# Example: Use mock API containers
import pytest
from testcontainers.compose import DockerCompose

@pytest.fixture(scope="session")
def docker_services():
    with DockerCompose(".", compose_file_name="docker-compose.test.yml") as compose:
        yield compose

def test_multi_agent_flow(docker_services):
    # Test against mock Shopify/Zendesk/Mailchimp
    pass
```

### Cost Monitoring (Phase 4-5)
```python
# Example: Cost validation in tests
def test_azure_resources_within_budget():
    # Use Azure Pricing API or Cost Management API
    # Assert monthly projected cost < $200
    pass
```

## Disaster Recovery Guidance

### RPO/RTO Targets
- **RPO:** 1 hour (max data loss acceptable)
- **RTO:** 4 hours (max recovery time)

### Backup Strategy (Phase 4-5)
- **Cosmos DB:** Continuous backup, point-in-time restore (30 days)
- **Terraform State:** Azure Blob Storage with versioning (30 day retention)
- **Container Images:** All production images retained indefinitely
- **Key Vault:** Soft-delete (90 days) + purge protection enabled

### DR Testing Schedule
- **Monthly:** Cosmos DB restore validation
- **Quarterly:** Full Terraform destroy/rebuild drill
- **Annually:** Tabletop exercise for all 5 DR scenarios

## Code Style and Conventions

### Python
- **Formatter:** black (line length 100)
- **Linter:** flake8
- **Type Hints:** Use for public APIs, optional for internal
- **Docstrings:** Google style, focus on "why" not "what"
- **Async:** Use async/await for AGNTCY SDK calls

### Comments
- Write for blog readers (educational audience)
- Explain architectural decisions and trade-offs
- Highlight cost optimization rationale
- Reference PROJECT-README.txt sections when relevant

### Naming
- Agents: `{purpose}_agent.py` (e.g., `intent_classification_agent.py`)
- Topics: `{agent-name}` (e.g., `intent-classifier`, `response-generator-en`)
- Containers: `{agent-name}:v{version}-{commit-sha}` (e.g., `intent-classifier:v1.0.0-abc123`)
- Environment vars: `UPPER_SNAKE_CASE` (e.g., `SLIM_ENDPOINT`, `COSMOS_CONNECTION_STRING`)

## Observability

### Phase 1-3 (Local)
- **Stack:** Grafana + ClickHouse + OpenTelemetry Collector (Docker Compose)
- **Endpoints:** Grafana (localhost:3001), OTLP (localhost:4318)
- **Tracing:** Enable via `AgntcyFactory(enable_tracing=True)`

### Phase 4-5 (Azure)
- **Stack:** Azure Monitor + Application Insights + Log Analytics
- **Retention:** 7 days (cost optimization)
- **Metrics:** Agent performance, response times, routing accuracy, costs
- **Alerts:** Latency >2min, error rate >5%, cost >80% budget

## Security Checklist

- [ ] All secrets in Azure Key Vault (Phase 4-5) or .env (Phase 1-3)
- [ ] .env files in .gitignore (never commit secrets)
- [ ] Managed identities for all Azure services (Phase 4-5)
- [ ] TLS 1.3 for all connections
- [ ] Network isolation for backend services (private endpoints)
- [ ] Pre-commit hooks: git-secrets, detect-secrets
- [ ] Dependency scanning: Dependabot, Snyk
- [ ] Secrets rotation procedures documented (quarterly)
- [ ] OWASP ZAP scanning in CI/CD pipeline

## Educational Considerations

This project is designed for a blog post audience. When writing code or documentation:

1. **Clarity over Brevity:** Prefer readable code with explanatory comments
2. **Cost Awareness:** Always explain cost implications of architectural choices
3. **Best Practices:** Demonstrate enterprise patterns (BCDR, IaC, CI/CD) even within budget
4. **Learning Path:** Code should be approachable for developers new to multi-agent systems
5. **Real-World Relevance:** Use realistic scenarios and data (e-commerce, customer service)
6. **Reproducibility:** All steps should be reproducible by blog readers on their own systems

## Common Questions & Answers

**Q: Why not use Azure Functions instead of Container Instances?**
A: Container Instances provide better isolation for long-running agents and align with AGNTCY SDK's design. Functions would require significant adaptation and may not support persistent agent sessions effectively.

**Q: Why Cosmos Serverless instead of provisioned throughput?**
A: Serverless mode provides pay-per-request billing, better for variable/unpredictable workloads, and easier to stay within $200/month budget. Provisioned RU/s has minimum costs even during idle periods.

**Q: Why not use managed Kubernetes (AKS)?**
A: AKS minimum costs (~$70-100/month for control plane + nodes) consume too much of the $200 budget. Container Instances provide adequate orchestration for 5 agents at much lower cost.

**Q: Can we use Azure OpenAI Service?**
A: Yes, but monitor token usage closely. Estimate $20-50/month for testing. Consider using canned responses in Phase 1-3 to avoid costs during development.

**Q: Why SLIM instead of NATS for transport?**
A: SLIM provides enterprise-grade security features out of the box. NATS is excellent for throughput but requires additional security configuration. For educational purposes, SLIM demonstrates secure patterns more clearly.

**Q: How do we handle the Zendesk cost if trial expires?**
A: Three options: (1) Use Zendesk Sandbox for partners/developers (free), (2) Budget $19-49/month and reduce Azure spend to $150-180, (3) Replace with free alternative like osTicket or FreshDesk free tier.

## When Stuck or Uncertain

1. **Check PROJECT-README.txt** for project requirements and constraints
2. **Review AGNTCY-REVIEW.md** for SDK integration patterns
3. **Verify budget impact** for any Azure service suggestion (Phase 4-5)
4. **Ask clarifying questions** about requirements before implementing
5. **Suggest alternatives** with cost/benefit trade-offs
6. **Document decisions** in code comments for blog readers
7. **Test locally first** before deploying to Azure (Phase 1-3 patterns apply to Phase 4-5)

## Success Criteria

The project is successful if:
- ‚úÖ Phase 1-3 runs entirely locally with $0 cloud costs
- ‚úÖ Phase 4-5 stays within $310-360/month Azure budget (REVISED 2026-01-22)
- ‚úÖ All 6 agents communicate via AGNTCY SDK successfully (REVISED: added Critic/Supervisor)
- ‚úÖ KPIs are measurable and demonstrable (even with mock data in Phase 1-3)
- ‚úÖ Code is educational, well-documented, and reproducible
- ‚úÖ Disaster recovery procedures are tested and validated
- ‚úÖ Security best practices are followed (secrets management, encryption, network isolation, PII tokenization)
- ‚úÖ CI/CD pipelines work reliably (GitHub Actions ‚Üí Azure DevOps)
- ‚úÖ Blog readers can follow along and build it themselves
- ‚úÖ Content validation (Critic/Supervisor) blocks malicious inputs and harmful outputs
- ‚úÖ Execution tracing provides full decision tree visibility for operators

## Final Notes

- **Priority Order:** Cost constraints > Functionality > Performance > Feature richness
- **Mindset:** This is a learning project - optimize for clarity and education
- **Budget:** $310-360/month is a HARD LIMIT for Phase 4-5 (REVISED 2026-01-22), with post-Phase 5 target of $200-250/month
- **Timeline:** No time estimates - focus on what needs to be done, not when
- **Audience:** Developers learning multi-agent AI, Azure, and cost optimization techniques

When in doubt, optimize for:
1. Educational value (blog readers learning)
2. Cost efficiency (demonstrate real-world budget constraints)
3. Reproducibility (readers can build it themselves)
4. Best practices (even within tight constraints)

---

## Recent Updates

### 2026-01-26: Azure OpenAI Integration COMPLETE ‚úÖ
- ‚úÖ Created shared Azure OpenAI client module (`shared/azure_openai.py`)
- ‚úÖ Updated all 5 agents to use Azure OpenAI with fallback to mock responses
- ‚úÖ Created cost monitoring module (`shared/cost_monitor.py`)
- ‚úÖ Integrated Phase 3.5 production prompts into agents
- ‚úÖ Added integration tests for Azure OpenAI connectivity
- ‚úÖ Updated requirements.txt with Azure OpenAI dependencies

**Agents Updated:**
| Agent | Model | Use Case |
|-------|-------|----------|
| Intent Classification | gpt-4o-mini | 17 intent categories (98% accuracy prompt) |
| Critic/Supervisor | gpt-4o-mini | Input/output validation (0% FP prompt) |
| Response Generation | gpt-4o | Natural customer responses (88.4% quality prompt) |
| Escalation | gpt-4o-mini | Human escalation detection (100% precision prompt) |
| Knowledge Retrieval | text-embedding-3-large | Semantic search (RAG) |

**New Files:**
- `shared/azure_openai.py` - Unified Azure OpenAI client with token tracking
- `shared/cost_monitor.py` - Budget alerts and usage reporting
- `tests/integration/test_azure_openai.py` - Integration test suite
- `docs/AZURE-OPENAI-INTEGRATION-SUMMARY.md` - Implementation documentation

**Environment Variables:**
```bash
AZURE_OPENAI_ENDPOINT=https://your-resource.openai.azure.com/
AZURE_OPENAI_API_KEY=your-api-key
USE_AZURE_OPENAI=true
```

**See:** `docs/AZURE-OPENAI-INTEGRATION-SUMMARY.md` for complete details

### 2026-01-26: Phase 4 Container Deployment COMPLETE ‚úÖ
- ‚úÖ All 8 container groups deployed and running in Azure
- ‚úÖ Created Critic/Supervisor agent (agents/critic_supervisor/)
- ‚úÖ Built and pushed all container images to ACR
- ‚úÖ Fixed SLIM gateway startup issue (explicit command required)
- ‚úÖ Created comprehensive deployment knowledge base (docs/PHASE-4-DEPLOYMENT-KNOWLEDGE-BASE.md)

**Container Groups Deployed:**
| Container | Private IP | Status |
|-----------|------------|--------|
| SLIM Gateway | 10.0.1.4:8443 | ‚úÖ Succeeded |
| NATS JetStream | 10.0.1.5:4222 | ‚úÖ Succeeded |
| Knowledge Retrieval | 10.0.1.6:8080 | ‚úÖ Succeeded |
| Critic/Supervisor | 10.0.1.7:8080 | ‚úÖ Succeeded |
| Response Generator | 10.0.1.8:8080 | ‚úÖ Succeeded |
| Analytics | 10.0.1.9:8080 | ‚úÖ Succeeded |
| Intent Classifier | 10.0.1.10:8080 | ‚úÖ Succeeded |
| Escalation | 10.0.1.11:8080 | ‚úÖ Succeeded |

**Issues Resolved:**
1. **SLIM "no command specified"** - Added `commands = ["/slim", "--port", "8443"]` to Terraform
2. **ACR push network errors** - Resolved with retries
3. **Missing Critic/Supervisor agent** - Created complete implementation

**See:** `docs/PHASE-4-DEPLOYMENT-KNOWLEDGE-BASE.md` for complete troubleshooting guide

### 2026-01-26: Phase 4 Azure Infrastructure Deployed
- ‚úÖ Created complete Terraform configuration in `terraform/phase4_prod/`
- ‚úÖ Deployed Azure OpenAI models (gpt-4o, gpt-4o-mini, text-embedding-3-large)
- ‚úÖ Deployed VNet with container and private endpoint subnets
- ‚úÖ Deployed Cosmos DB Serverless with 5 containers (sessions, messages, analytics, pii-tokens, documents)
- ‚úÖ Deployed Key Vault with RBAC and Azure OpenAI API key stored
- ‚úÖ Deployed Container Registry (ACR Basic)
- ‚úÖ Deployed Application Insights with 50% sampling
- ‚úÖ Deployed Log Analytics with 30-day retention
- ‚úÖ Configured private endpoints for Cosmos DB and Key Vault
- ‚úÖ Set up budget alerts at 83% ($299) and 93% ($335)
- ‚úÖ Created managed identity for container access

**Deployed Resources:**
- VNet: `agntcy-cs-prod-vnet` (10.0.0.0/16)
- Cosmos DB: `cosmos-agntcy-cs-prod-rc6vcp`
- Key Vault: `kv-agntcy-cs-prod-rc6vcp`
- ACR: `acragntcycsprodrc6vcp.azurecr.io`
- App Insights: `agntcy-cs-prod-appi-rc6vcp`

### 2026-01-26: UCP Integration Evaluation COMPLETE
- ‚úÖ Evaluated Universal Commerce Protocol (UCP) for Phase 4-5 adoption
- ‚úÖ Compared AGNTCY multi-agent architecture vs Shopify shop-chat-agent
- ‚úÖ Assessed implementation complexity (80-120 hours, +$15-25/month)
- ‚úÖ Designed MCP/UCP testing and validation approach
- ‚úÖ Created comprehensive wiki documentation

**Key Documents Created:**
- `docs/EVALUATION-Universal-Commerce-Protocol-UCP.md` - Strategic evaluation (ADOPT recommendation)
- `docs/UCP-IMPLEMENTATION-COMPLEXITY-ANALYSIS.md` - Component-by-component effort analysis
- `docs/MCP-TESTING-VALIDATION-APPROACH.md` - Testing pyramid and validation strategy
- `docs/WIKI-UCP-Integration-Guide.md` - Wiki page with benefits and use cases
- `docs/COMPARISON-Shopify-Shop-Chat-Agent.md` - Architecture comparison

**Decision:** PROCEED with UCP adoption (Score: 4.15/5)
- Phase 4: MCP client, Catalog, Embedded Checkout (52-78 hours)
- Phase 5: Native Checkout, Extensions, AP2 Payments (56-80 hours)
- Budget impact: Within 10% variance tolerance

**Comparison Highlights (AGNTCY vs Shopify):**
| Dimension | AGNTCY | Shopify |
|-----------|--------|---------|
| Architecture | 6 specialized agents | Single Claude agent |
| Safety | Defense-in-depth | None documented |
| Cost tracking | Real-time per-message | None |
| Observability | Full execution traces | Console logging only |

### 2026-01-25: Phase 3.5 AI Model Optimization COMPLETE
- ‚úÖ Completed all 6 evaluation types with exit criteria exceeded
- ‚úÖ Intent Classification: 98% accuracy (3 iterations)
- ‚úÖ Critic/Supervisor: 0% FP, 100% TP (2 iterations)
- ‚úÖ Escalation Detection: 100% precision/recall (2 iterations)
- ‚úÖ RAG Retrieval: 100% retrieval@1 (1 iteration)
- ‚úÖ Response Quality: 88.4% quality score (2 iterations)
- ‚úÖ Robustness: 82% appropriateness (2 iterations)
- ‚úÖ Total cost: ~$0.10 (well under $20-50 budget)
- ‚úÖ 12 total iterations across all components

**Key Achievements:**
- Created 7 evaluation datasets (375 samples total)
- Created 5 production-ready prompts for Phase 4
- Added Register Matching to response prompt (fixed tone issues)
- Added Logic Manipulation detection to Critic prompt
- Added Sensitive Situations detection to Escalation prompt
- Model selection: GPT-4o-mini for all agents

**Key Deliverables:**
- `evaluation/test_harness.py` - Complete evaluation framework
- `evaluation/datasets/` - 7 datasets (375 samples)
- `evaluation/prompts/*_final.txt` - 5 production prompts
- `evaluation/results/` - 6 reports + completion summary

### 2026-01-22: Architecture Requirements & Budget Revision
- ‚úÖ Added 4 new architectural requirements (PII tokenization, data abstraction, event-driven, RAG)
- ‚úÖ Added Critic/Supervisor Agent (6th agent) for content validation
- ‚úÖ Added execution tracing & observability with OpenTelemetry
- ‚úÖ Created 5 new GitHub issues for event-driven features (#139-#143)
- ‚úÖ Revised budget to $310-360/month (from $200/month) to accommodate new requirements
- ‚úÖ Created comprehensive documentation:
  - `docs/data-staleness-requirements.md`
  - `docs/event-driven-requirements.md`
  - `docs/architecture-requirements-phase2-5.md`
  - `docs/critic-supervisor-agent-requirements.md`
  - `docs/execution-tracing-observability-requirements.md`
  - `docs/WIKI-Architecture.md` (GitHub wiki ready)
  - `docs/WIKI-PUBLISHING-INSTRUCTIONS.md`
- ‚è≥ GitHub issues for Critic/Supervisor and Tracing to be created
- ‚è≥ Wiki architecture document to be updated with new components

**Earlier (2026-01-22): GitHub Project Management Integration Complete**
- ‚úÖ Created 137 GitHub issues (7 epics + 130 user stories)
- ‚úÖ Configured 30 labels across 5 categories
- ‚úÖ Created 5 phase-based milestones with due dates
- ‚úÖ Automated issue creation via PowerShell scripts (100% success rate)
- ‚úÖ Phase 1 marked as 100% complete
- ‚úÖ Phase 2 readiness document prepared (PHASE-2-READINESS.md)

**All work documented in**:
- PROJECT-SETUP-COMPLETE.md (complete GitHub setup summary)
- PHASE-2-READINESS.md (Phase 2 requirements and work breakdown)
- user-stories-phased.md (complete user story catalog)

---

**Last Updated:** 2026-01-26
**Project Phase:** Phase 4 Infrastructure + Containers Deployed ‚úÖ
**Current Budget Status:** Full infrastructure running (~$214-285/month estimated)
**Phase 4-5 Budget:** $310-360/month (+$15-25 for UCP)
**Agent Count:** 6 agents deployed (Intent, Knowledge, Response, Escalation, Analytics, Critic/Supervisor)
**Production Prompts Ready:** 5 (intent, critic, escalation, response, judge)
**UCP Adoption:** Approved for Phase 4-5 (80-120 hours, MCP binding approach)
**Azure Resources:** VNet, Cosmos DB, Key Vault, ACR, App Insights, Private Endpoints, 8 Container Groups
**Container IPs:** SLIM (10.0.1.4), NATS (10.0.1.5), Agents (10.0.1.6-11)
**GitHub Project**: https://github.com/orgs/Remaker-Digital/projects/1
