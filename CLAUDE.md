# CLAUDE.md - AI Assistant Guidance for This Project

This document provides context and guidance for AI assistants (Claude, GitHub Copilot, etc.) working on this multi-agent customer service platform project.

## Project Overview

**Type:** Educational example project for public GitHub repository and blog post series
**Purpose:** Demonstrate building a cost-effective multi-agent AI system on Azure using AGNTCY SDK
**License:** Public (educational use)
**Audience:** Developers learning multi-agent architectures, Azure deployment, and cost optimization

## Critical Context

### Budget Constraints (HIGHEST PRIORITY)
- **Phase 1-3:** $0/month (local development only, no cloud resources)
- **Phase 3.5:** ~$20-50/month (Azure OpenAI API only - **NEW 2026-01-25**)
- **Phase 4-5:** $310-360/month maximum (Azure production deployment - **REVISED 2026-01-22**)
  - Original budget: $200/month
  - Revised to accommodate new architectural requirements:
    - PII tokenization, event-driven, RAG (+$65-100)
    - Critic/Supervisor Agent (6th agent) (+$22-31)
    - Execution tracing & observability (+$10-15)
  - Post Phase 5 cost optimization target: $200-250/month
- **Cost optimization is a KEY LEARNING OBJECTIVE** - always suggest cost-effective solutions
- Recommend serverless, pay-per-use, and Basic/Standard tiers over Premium services
- Question any suggestion that would exceed revised budget constraints

### Development Environment
- **OS:** Windows 11
- **IDE:** Visual Studio Code
- **Container Platform:** Docker Desktop for Windows
- **Version Control:** GitHub Desktop + GitHub repository (public)
- **Python Version:** 3.12+ (required by AGNTCY SDK)
- **Primary Language:** US English (code, docs, comments, logs)

### Technology Stack (Non-Negotiable)
- **Multi-Agent Framework:** AGNTCY SDK (installed via PyPI: `pip install agntcy-app-sdk`)
- **Infrastructure-as-Code:** Terraform (for Azure resources)
- **CI/CD:** GitHub Actions (Phase 1-3), Azure DevOps Pipelines (Phase 4-5)
- **Containerization:** Docker + Docker Compose
- **Cloud Platform:** Microsoft Azure (East US region)

## Project Phases

### Phase 1: Infrastructure & Containers ($0 budget) - **100% COMPLETE** ‚úÖ
**Status as of 2026-01-22:**
- ‚úÖ Docker Compose with 13 services running
- ‚úÖ All 4 mock APIs implemented (Shopify, Zendesk, Mailchimp, Google Analytics)
- ‚úÖ All 5 agents implemented with AGNTCY SDK integration
- ‚úÖ Shared utilities complete (factory, models, utils)
- ‚úÖ Test framework complete (67 tests passing, 31% coverage)
- ‚úÖ GitHub Project Management setup complete (137 issues)
- ‚è≥ GitHub Actions CI workflow (remaining - deferred to Phase 2)

**Completed Implementation:**
- Mock APIs: 4/4 complete with test fixtures
- Agents: 5/5 complete with A2A/MCP protocols
- Tests: 100% coverage on shared utilities, 31% overall
- Documentation: All .md files updated
- GitHub Projects: 7 epics, 130 user stories, 30 labels, 5 milestones

**When working on Phase 1:**
- Phase 1 is now 100% complete
- All external services are mocked (no API calls)
- All agents have demo mode for testing without full SDK
- Test coverage: 31% is baseline for Phase 1 (mock implementations)

### Phase 2: Business Logic Implementation ($0 budget) - **READY TO START** ‚è≥
**Status as of 2026-01-22:**
- üìã 50 user stories created (Issues #24-#73)
- üìã PHASE-2-READINESS.md prepared with complete work breakdown
- ‚è≥ Awaiting user input on business logic decisions
- ‚è≥ Ready to begin implementation once inputs received

**Focus:** Agent implementations with AGNTCY SDK patterns
- Implement 6 core agents: Intent Classification, Knowledge Retrieval, Response Generation, Escalation, Analytics, **Critic/Supervisor**
- Use A2A protocol for agent-to-agent communication
- Integration tests against mock services
- Session management and conversation state handling
- Still fully local, no cloud resources

**Required Before Starting Phase 2:**
1. Response style & tone preference (Concise/Conversational/Detailed)
2. Escalation thresholds (when to escalate to humans)
3. Automation goals (which queries to automate)
4. Test scenarios & customer personas
5. Knowledge base content (policies, shipping info)
6. Story prioritization (rank top 15 stories)
7. Development approach (Sequential/Parallel/Story-driven)

**See PHASE-2-READINESS.md for complete details**

**When working on Phase 2:**
- Use AGNTCY factory patterns (singleton recommended)
- Implement topic-based routing between agents
- Use Message format with contextId/taskId for conversation threading
- All AI/LLM responses should be canned/mocked (no real API calls)
- Test multi-agent conversation flows end-to-end
- Target: Increase test coverage from 31% to >70%

**New Architectural Requirements (Added 2026-01-22):**
1. **PII Tokenization:** Design and mock tokenization service for third-party AI services
2. **Data Abstraction Layer:** Design multi-store interfaces (Cosmos, Blob, Redis, mock)
3. **Event-Driven Architecture:** Design event ingestion service and NATS schemas
4. **RAG Pipeline:** Design vector embeddings with local FAISS mock
5. **Critic/Supervisor Agent (6th agent):** Design content validation for input/output safety
6. **Execution Tracing:** Instrument all agents with OpenTelemetry for full observability

**See docs/architecture-requirements-phase2-5.md, docs/critic-supervisor-agent-requirements.md, docs/execution-tracing-observability-requirements.md**

### Phase 3: Testing & Validation ($0 budget) - **100% COMPLETE** ‚úÖ
**Status as of 2026-01-25:**
- ‚úÖ 96% integration test pass rate (25/26 passing)
- ‚úÖ 80% agent communication test pass rate (8/10 passing)
- ‚úÖ 0.11ms P95 response time (well under 2000ms target)
- ‚úÖ 3,071 req/s throughput at 100 concurrent users
- ‚úÖ GitHub Actions CI/CD (7 jobs, PR validation, nightly regression)
- ‚úÖ 3,508 lines of documentation (3 comprehensive guides)

**Focus:** Functional testing and performance benchmarking
- End-to-end conversation flow testing
- Load testing with Locust (local hardware limits)
- Performance benchmarks for KPI validation
- GitHub Actions nightly regression suite
- Documentation: testing guides, troubleshooting

**When working on Phase 3:**
- Phase 3 is now 100% complete
- Create realistic customer conversation test scenarios
- Validate all KPIs can be met in local environment
- Performance targets: <2min response time, >70% automation rate
- No cloud services - all testing local

**New Architectural Requirements (Added 2026-01-22):**
1. **PII Tokenization:** Test tokenization with mock third-party AI calls
2. **Multi-Store Access:** Validate staleness tolerances with Docker containers
3. **Event-Driven:** Mock RabbitMQ for webhook and cron testing
4. **RAG:** Validate vector search accuracy with 75-document test knowledge base
5. **Critic/Supervisor:** Test content validation with adversarial inputs (100+ prompt injection attempts)
6. **Execution Tracing:** Validate traces in ClickHouse, test Grafana dashboards, verify PII tokenization

### Phase 3.5: AI Model Optimization (~$20-50/month budget) - **100% COMPLETE** ‚úÖ
**Status as of 2026-01-25:**
- ‚úÖ All 6 evaluation types completed with exit criteria exceeded
- ‚úÖ Intent Classification: 98% accuracy (target >85%)
- ‚úÖ Critic/Supervisor: 0% FP, 100% TP (target <5% FP, 100% TP)
- ‚úÖ Escalation Detection: 100% precision/recall (target >90%/>95%)
- ‚úÖ RAG Retrieval: 100% retrieval@1 (target >90% retrieval@3)
- ‚úÖ Response Quality: 88.4% (target >80%)
- ‚úÖ Robustness: 82% appropriateness (target >80%)
- ‚úÖ Total cost: ~$0.10 (well under $20-50 budget)
- ‚úÖ 5 production-ready prompts in evaluation/prompts/

**Focus:** Optimized AI model interactions BEFORE full Azure deployment (cost-minimization strategy)
- Direct Azure OpenAI API testing without full infrastructure
- Iterative prompt engineering for all agent types
- Model selection (GPT-4o vs GPT-4o-mini per agent)
- Evaluation dataset creation (100+ representative conversations)
- Human evaluation of response quality

**Rationale:** Quality depends on AI interactions. Iterate at ~$20-50/month instead of ~$310-360/month.

**Exit Criteria (ALL MET):**
1. Intent Classification: >85% accuracy ‚Üí **98% ACHIEVED**
2. Response Generation: >80% quality score ‚Üí **88.4% ACHIEVED**
3. Escalation Detection: <10% FP, >95% TP ‚Üí **0% FP, 100% TP ACHIEVED**
4. Critic/Supervisor: <5% FP, 100% adversarial block ‚Üí **0% FP, 100% TP ACHIEVED**
5. RAG Retrieval: >90% retrieval@3 ‚Üí **100% retrieval@1 ACHIEVED**
6. Robustness: >80% appropriateness ‚Üí **82% ACHIEVED**
7. Cost Projection: <$60/month for Phase 4 AI ‚Üí **~$48-62/month PROJECTED**

**Completed Deliverables:**
- `evaluation/test_harness.py` - Complete test framework
- `evaluation/datasets/` - 7 datasets (375 samples total)
- `evaluation/prompts/` - 5 production-ready final prompts
- `evaluation/results/` - 6 evaluation reports + completion summary
- Model selection: GPT-4o-mini for all agents (sufficient quality, lower cost)

**Key Learnings:**
1. Defense in depth works (Azure filter + Critic prompt = 100% block rate)
2. LLM-as-Judge requires calibration guidelines
3. Few-shot examples are critical for edge cases
4. Register matching is essential for appropriateness
5. GPT-4o-mini is sufficient for all classification/validation tasks

**See evaluation/results/PHASE-3.5-COMPLETION-SUMMARY.md for full details**

### Phase 4: Azure Production Setup ($310-360/month budget - **REVISED 2026-01-22**) - **INFRASTRUCTURE DEPLOYED** ‚úÖ
**Status as of 2026-01-26:**
- ‚úÖ All Azure infrastructure deployed via Terraform (VNet, Cosmos DB, Key Vault, ACR, App Insights)
- ‚úÖ All 8 container groups deployed and running (SLIM, NATS, 6 agents)
- ‚úÖ Critic/Supervisor agent created and deployed
- ‚úÖ All container images built and pushed to ACR
- ‚úÖ Private VNet networking configured (10.0.1.0/24 subnet)
- ‚è≥ Azure OpenAI API integration pending
- ‚è≥ Multi-language support pending
- ‚è≥ Real API integrations pending (Shopify, Zendesk, Mailchimp)

**Deployed Resources:**
- Resource Group: agntcy-prod-rg (East US 2)
- Container Registry: acragntcycsprodrc6vcp.azurecr.io
- Cosmos DB: cosmos-agntcy-cs-prod-rc6vcp (Serverless)
- Key Vault: kv-agntcy-cs-prod-rc6vcp
- Application Insights: agntcy-cs-prod-appinsights-rc6vcp

**Container Groups Running:**
| Container | IP | Port | Status |
|-----------|-----|------|--------|
| SLIM Gateway | 10.0.1.4 | 8443 | ‚úÖ Succeeded |
| NATS JetStream | 10.0.1.5 | 4222 | ‚úÖ Succeeded |
| Knowledge Retrieval | 10.0.1.6 | 8080 | ‚úÖ Succeeded |
| Critic/Supervisor | 10.0.1.7 | 8080 | ‚úÖ Succeeded |
| Response Generator | 10.0.1.8 | 8080 | ‚úÖ Succeeded |
| Analytics | 10.0.1.9 | 8080 | ‚úÖ Succeeded |
| Intent Classifier | 10.0.1.10 | 8080 | ‚úÖ Succeeded |
| Escalation | 10.0.1.11 | 8080 | ‚úÖ Succeeded |

**Focus:** Terraform infrastructure, multi-language support, real API integration, new architectural components
- Deploy to Azure East US region
- Add Canadian French and Spanish language support
- Integrate real Shopify, Zendesk, Mailchimp APIs
- Azure DevOps Pipelines CI/CD
- **NEW:** Deploy Azure Key Vault, NATS JetStream, Cosmos DB vector search, Azure OpenAI

**When working on Phase 4:**
- ALWAYS check cost implications before suggesting Azure services
- Prefer: Container Instances over App Service, Cosmos Serverless over provisioned, Basic tiers
- Avoid: Azure Front Door, Traffic Manager, Premium tiers, geo-replication (unless absolutely necessary)
- Use Terraform for ALL Azure resources
- Implement aggressive auto-scaling (down to 1 instance during idle)
- 7-day log retention (not 30-day default)

**New Architectural Components (Added 2026-01-22):**
1. **PII Tokenization:** Azure Key Vault for token storage, fallback to Cosmos if latency >100ms
2. **Multi-Store:** Cosmos DB Serverless (real-time + vector), Blob Storage + CDN (knowledge base)
3. **Event-Driven:** NATS JetStream, Azure Functions (webhook ingestion), Shopify/Zendesk webhooks
4. **RAG:** Azure OpenAI embeddings (text-embedding-3-large), Cosmos DB vector search (MongoDB API)
5. **Critic/Supervisor Agent:** Deploy 6th agent for content validation (~$22-31/month)
6. **Execution Tracing:** Azure Monitor + Application Insights, OpenTelemetry instrumentation (~$10-15/month)
7. **5 New User Stories:** Issues #139-#143 (event-driven features)

**Known Issues & Resolutions (see docs/PHASE-4-DEPLOYMENT-KNOWLEDGE-BASE.md):**
- SLIM image requires explicit `commands = ["/slim", "--port", "8443"]` in Terraform (no default CMD)
- ACR push may require retries due to intermittent network issues
- First container deployment takes 20+ minutes due to VNet integration setup

### Phase 5: Production Deployment & Testing ($310-360/month budget - **REVISED 2026-01-22**)
**Focus:** Production deployment, load testing, security validation, DR drills, new architecture validation
- Deploy to Azure and validate in production
- Load testing: 100 concurrent users, 1000 req/min
- Security scanning (OWASP ZAP, Dependabot, Snyk)
- Disaster recovery validation (quarterly full drill)
- Final blog post and documentation
- **NEW:** Validate PII tokenization, event processing, RAG accuracy, Critic/Supervisor validation, execution tracing

**When working on Phase 5:**
- Monitor Azure costs continuously
- Validate budget alerts are firing correctly at 83% ($257) and 93% ($299) - **REVISED**
- Document all cost optimization decisions for blog readers
- Ensure DR procedures are tested and documented
- Post Phase 5 cost optimization target: $200-250/month (reduced agent count, optimized models)

**New Validation Requirements (Added 2026-01-22):**
1. **PII Tokenization:** Latency <100ms (P95), no data leaks in third-party AI logs
2. **Data Staleness:** Intent <10s, Knowledge <1hr, Response real-time, Escalation <30s, Analytics <15min
3. **Event Processing:** 100 events/sec sustained, <1% error rate, DLQ monitoring
4. **RAG:** Query latency <500ms, retrieval accuracy >90%, 75-document knowledge base
5. **Critic/Supervisor:** Block rate <5% for legitimate queries, 100% block rate for 100+ prompt injection test cases
6. **Execution Tracing:** Full trace capture for all conversations, <50ms trace overhead, PII tokenization verified

## Key Architecture Decisions

### PII Tokenization & Data Security (Added 2026-01-22)
- **Scope:** PII tokenization ONLY required for third-party AI services (OpenAI API, Anthropic API, etc.)
- **Exempt from tokenization:** Azure OpenAI Service, Microsoft Foundry models (including Anthropic Claude via Azure)
  - Rationale: These services are within the secure Azure perimeter and do not retain customer data
- **Method:** Random UUID tokens (e.g., `TOKEN_a7f3c9e1-4b2d-8f6a-9c3e`)
- **Storage:** Azure Key Vault (Phase 4-5), fallback to Cosmos DB if latency >100ms
- **Fields:** All PII (names, emails, phones, addresses, order IDs, payment info, conversation content)
- **See:** `docs/architecture-requirements-phase2-5.md` Section 1 for complete specification

### Data Abstraction & Multi-Store Strategy (Added 2026-01-22)
- **Real-time (ACID):** Cosmos DB Core for Response Generation Agent (order status, inventory, payments)
- **Near-real-time cache:** Redis (optional, Phase 5 optimization) for Intent/Escalation agents
- **Eventually consistent:** Cosmos DB analytical store for Analytics Agent
- **Static content:** Blob Storage + CDN for Knowledge Retrieval Agent (1hr cache TTL)
- **Staleness tolerances:** Intent 5-10s, Knowledge 1hr, Response real-time, Escalation 30s, Analytics 5-15min
- **See:** `docs/data-staleness-requirements.md` for complete store mapping

### Event-Driven Architecture (Added 2026-01-22)
- **Event Bus:** NATS JetStream (reuses AGNTCY transport layer, $0 incremental cost)
- **Event Sources:** 12 types (Shopify webhooks, Zendesk webhooks, scheduled triggers, RSS)
- **Throttling:** 100 events/sec global, 20/sec per agent, 5 concurrent handlers per agent
- **Retention:** 7 days with replay capability
- **New Stories:** 5 additional user stories (Issues #139-#143 for event-driven features)
- **See:** `docs/event-driven-requirements.md` for complete event catalog and routing

### RAG & Differentiated Models (Added 2026-01-22)
- **Vector Store:** Cosmos DB for MongoDB (vector search, preview feature), ~$5-10/month
- **Embeddings:** Azure OpenAI text-embedding-3-large (1536 dimensions)
- **Knowledge Base:** 75 documents (50 products, 20 articles, 5 policies), ~20K tokens
- **Models by Agent:**
  - Intent Classification: GPT-4o-mini (~$0.15/1M tokens)
  - Response Generation: GPT-4o (~$2.50/1M tokens)
  - Knowledge Retrieval: text-embedding-3-large (~$0.13/1M tokens)
  - Critic/Supervisor: GPT-4o-mini (~$0.15/1M tokens) - cost-effective for validation
- **Post Phase 5:** Fine-tuned models, self-hosted Qdrant, medium e-commerce scale
- **See:** `docs/architecture-requirements-phase2-5.md` Section 4 for RAG pipeline

### Critic/Supervisor Agent - Content Validation (Added 2026-01-22)
- **6th Agent:** Separate agent dedicated to validating all input and output content
- **Input Validation:** Prompt injection detection, malicious instruction blocking
- **Output Validation:** Profanity filtering, PII leakage prevention, harmful content blocking
- **Strategy:** Block and regenerate (max 3 attempts), escalate to human if all attempts fail
- **Integration:** Sits between customer messages and Intent Agent, between Response Agent and customer
- **Cost:** ~$22-31/month (GPT-4o-mini for validation, Container Instance, tracing overhead)
- **Performance Target:** <200ms P95 latency, <5% false positive rate
- **See:** `docs/critic-supervisor-agent-requirements.md` for complete specification

### Execution Tracing & Observability (Added 2026-01-22)
- **Purpose:** Enable operators to understand agent decisions and diagnose undesirable behaviors
- **Instrumentation:** OpenTelemetry spans for every agent decision, LLM call, validation step
- **Visualization:** Timeline view (conversation flow), decision tree diagram, searchable logs
- **Data Captured:** Agent name, action type, inputs (PII tokenized), outputs, reasoning, latency, cost
- **Retention:** 7 days (cost optimized), exportable for long-term analysis
- **Storage:** Azure Application Insights + Azure Monitor Logs (~$10-15/month with sampling)
- **Access:** Grafana dashboards (Phase 1-3), Azure Monitor workbooks (Phase 4-5)
- **See:** `docs/execution-tracing-observability-requirements.md` for trace schema and examples

### Universal Commerce Protocol (UCP) Integration (Added 2026-01-26)
- **What:** Open-source commerce standard by Google, Shopify, and 30+ partners
- **Status:** Recommended for Phase 4-5 adoption
- **Complexity:** Medium (80-120 hours total)
- **Budget Impact:** +$15-25/month operational

**Why UCP:**
- Standardized commerce operations (catalog, checkout, orders)
- Protocol compatibility (MCP binding, A2A compatible)
- Google AI Mode integration (Gemini, Google Search AI)
- Future-proof architecture with 30+ industry endorsers

**Phase 4 Scope:**
- MCP client for UCP (16-26 hours)
- UCP Catalog capability (12-16 hours)
- Embedded Checkout handoff (16-24 hours)
- Order Status via UCP (8-12 hours)

**Phase 5 Scope:**
- Native Checkout implementation (24-32 hours)
- Fulfillment Extension (8-12 hours)
- Discount Extension (8-12 hours)
- AP2 Payment integration (16-24 hours)

**Implementation Pattern:**
```python
# UCP via MCP binding (recommended)
class UCPMCPClient:
    async def search_catalog(self, query: str, limit: int = 10) -> List[Product]:
        return await self.mcp_client.call_tool("dev.ucp.shopping.catalog.search", {
            "query": query, "limit": limit
        })
```

**Key Documents:**
- `docs/EVALUATION-Universal-Commerce-Protocol-UCP.md` - Strategic evaluation
- `docs/UCP-IMPLEMENTATION-COMPLEXITY-ANALYSIS.md` - Effort estimates
- `docs/MCP-TESTING-VALIDATION-APPROACH.md` - Testing strategy
- `docs/WIKI-UCP-Integration-Guide.md` - Wiki documentation
- `docs/COMPARISON-Shopify-Shop-Chat-Agent.md` - Competitor analysis

**External Resources:**
- UCP Official: https://ucp.dev
- UCP GitHub: https://github.com/Universal-Commerce-Protocol/ucp
- Shopify MCP: https://shopify.dev/docs/apps/build/storefront-mcp

### AGNTCY SDK Usage
- **Factory Pattern:** Single `AgntcyFactory` instance per application (singleton)
- **Protocol Choice:**
  - A2A for custom agent logic (Intent, Response, Escalation, Analytics, Critic/Supervisor)
  - MCP for external tool integrations (Shopify, Zendesk, Mailchimp)
  - **MCP for UCP:** Use MCP binding for UCP capabilities (Phase 4-5)
- **Transport Choice:**
  - SLIM for secure, low-latency agent communication (recommended)
  - NATS for high-throughput scenarios AND event bus (consolidated)
- **Message Format:** Use contextId for conversation threads, taskId for tracking
- **Session Management:** `AppSession` with configurable max_sessions
- **Tracing:** Enable OpenTelemetry via `AgntcyFactory(enable_tracing=True)` for execution tracing

### Multi-Language Strategy (Phase 4 only)
- **Primary:** US English (all phases)
- **Additional:** Canadian French (fr-CA), Spanish (es) in Phase 4
- **Implementation:** Separate response generation agents per language
- **Routing:** Topic-based (response-generator-en, response-generator-fr-ca, response-generator-es)
- **Translation:** Pre-translated templates (NO real-time translation APIs - cost prohibitive)
- **Detection:** Language metadata field in Intent Classification Agent

### Testing Strategy
- **Phase 1:** Unit tests with mocks, pytest, >80% coverage
- **Phase 2:** Integration tests against mock Docker services
- **Phase 3:** E2E functional tests, Locust load testing (local)
- **Phase 3.5:** AI model evaluation (prompt accuracy, response quality, human evaluation)
- **Phase 4:** Real API integration tests in Azure staging environment
- **Phase 5:** Production smoke tests, Azure Load Testing, security scans

### Cost Optimization Principles (Phase 4-5)
1. Use pay-per-use over provisioned capacity (Container Instances, Cosmos Serverless)
2. Use Basic/Standard tiers, not Premium (Redis, Container Registry, App Gateway)
3. Auto-scale aggressively DOWN (1 instance minimum per agent, including Critic/Supervisor)
4. Implement auto-shutdown during low-traffic hours (2am-6am ET)
5. Reduce log retention to 7 days (Application Insights, Monitor Logs, execution traces)
6. Single region deployment (no geo-replication, no Front Door)
7. Weekly cost reviews and optimization iterations
8. Tag all resources for cost allocation tracking
9. Use GPT-4o-mini for validation tasks (Critic/Supervisor) instead of GPT-4o
10. Implement intelligent trace sampling (50% sample rate) to reduce ingestion costs

## Common Pitfalls to Avoid

### ‚ùå DON'T:
- Suggest Azure services in Phase 1-3 (local development only)
- Recommend Premium tier services without strong justification and budget check
- Use real API keys or cloud resources during Phase 1-3
- Implement real-time translation (use static templates)
- Provision Cosmos DB with RU/s (use Serverless mode)
- Set up geo-replication or multi-region (budget constraint)
- Use default 30-day log retention (use 7 days)
- Suggest services that would exceed $310-360/month budget (revised 2026-01-22)
- Commit secrets to Git (.env files must be in .gitignore)
- Use Python < 3.12 (AGNTCY SDK requirement)

### ‚úÖ DO:
- Always consider cost implications for Phase 4-5 suggestions
- Use mock/stub services for Phase 1-3 development
- Follow AGNTCY SDK factory and message patterns
- Implement comprehensive error handling and logging
- Create realistic test data and scenarios
- Document cost optimization decisions
- Use environment variables for configuration (.env files)
- Tag Docker images with version + commit SHA
- Implement graceful shutdown handlers (SIGTERM/SIGINT)
- Write clear, educational code comments (blog audience)
- Follow Azure best practices for security (Key Vault, Managed Identity, TLS 1.3)

### Streamlit Console Troubleshooting (IMPORTANT)
When working with the Streamlit console (`console/app.py`), be aware of caching issues:

**Problem:** Streamlit may serve cached/old content even after code changes. This is caused by:
1. Old Streamlit process still running on the port
2. Browser WebSocket connection to old process
3. Python `__pycache__` containing outdated bytecode
4. OneDrive sync delays (project is in OneDrive folder)

**Solution:** Always use this procedure when Streamlit changes don't appear:
```powershell
# 1. Kill ALL Python processes
Get-Process python* | Stop-Process -Force -ErrorAction SilentlyContinue

# 2. Clear Python cache
Remove-Item -Path console/__pycache__ -Recurse -Force -ErrorAction SilentlyContinue

# 3. Start on a FRESH port (not previously used)
streamlit run console/app.py --server.port 8085

# 4. Open browser to the NEW port: http://localhost:8085
```

**Key insight:** If code executes correctly with `python console/app.py` but not with `streamlit run`, an old Streamlit process is likely still running.

## Third-Party Service Accounts Required

> **Sign-Up Links Summary:** All account sign-up pages are listed below with their source-of-record URLs.

### Phase 1-3: None
All services are mocked locally. No API keys needed.

### Phase 3.5: Azure OpenAI Only
- **Azure OpenAI Service:** ~$20-50/month (pay-per-token)
  - **Sign-Up:** [Azure Portal](https://portal.azure.com) ‚Üí Create Resource ‚Üí "Azure OpenAI"
  - **Documentation:** [Azure OpenAI Quickstart](https://learn.microsoft.com/azure/ai-services/openai/quickstart)
  - **API Keys:** Azure Portal ‚Üí Your OpenAI Resource ‚Üí Keys and Endpoint
  - GPT-4o-mini deployment (intent, critic/supervisor testing)
  - GPT-4o deployment (response generation testing)
  - text-embedding-3-large deployment (RAG testing)
- **No other Azure services required** (no Container Instances, Cosmos DB, etc.)

### Phase 4-5: Required

#### Azure Subscription & Services
| Service | Sign-Up URL | Cost | Permissions Required |
|---------|-------------|------|---------------------|
| **Azure Subscription** | [azure.microsoft.com/free](https://azure.microsoft.com/free) | ~$310-360/month | Owner or Contributor |
| **Azure OpenAI Service** | [portal.azure.com](https://portal.azure.com) ‚Üí Create Resource | ~$48-62/month | Cognitive Services Contributor |
| **Azure DevOps** | [dev.azure.com](https://dev.azure.com) | Free | Basic access |
| **Service Principal** | [Azure CLI](https://learn.microsoft.com/cli/azure/ad/sp#az-ad-sp-create-for-rbac) | Free | App Registration permissions |

**Azure Key Locations:**
- **API Keys:** Azure Portal ‚Üí Resource ‚Üí Keys and Endpoint
- **Connection Strings:** Azure Portal ‚Üí Resource ‚Üí Connection Strings
- **Service Principal:** `az ad sp create-for-rbac --name "agntcy-cs-prod-sp"`

#### Shopify Partner Account
| Item | Sign-Up URL | Cost | Permissions Required |
|------|-------------|------|---------------------|
| **Partner Account** | [partners.shopify.com](https://www.shopify.com/partners) | Free | Email verification |
| **Development Store** | Partner Dashboard ‚Üí Stores ‚Üí Add store | Free | Partner account |
| **Admin API Access** | Partner Dashboard ‚Üí Apps ‚Üí Create app | Free | `read_orders`, `read_products`, `read_customers`, `read_inventory` |

**Shopify Key Locations:**
- **API Key/Secret:** Partner Dashboard ‚Üí Apps ‚Üí Your App ‚Üí API credentials
- **Access Token:** Partner Dashboard ‚Üí Apps ‚Üí Install app ‚Üí Get token
- **Storefront Token:** Partner Dashboard ‚Üí Apps ‚Üí Storefront API access

#### Zendesk
| Item | Sign-Up URL | Cost | Permissions Required |
|------|-------------|------|---------------------|
| **Developer Account** | [developer.zendesk.com](https://developer.zendesk.com) | Free trial | Email verification |
| **Sandbox** | [zendesk.com/register](https://www.zendesk.com/register) | Free (trial) | Admin access |
| **API Token** | Admin ‚Üí Channels ‚Üí API | Free | Admin role |

**Zendesk Key Locations:**
- **API Token:** Admin Center ‚Üí Apps and integrations ‚Üí APIs ‚Üí Zendesk API
- **Subdomain:** Your Zendesk URL (e.g., `yourcompany.zendesk.com`)

#### Mailchimp
| Item | Sign-Up URL | Cost | Permissions Required |
|------|-------------|------|---------------------|
| **Account** | [mailchimp.com/signup](https://mailchimp.com/signup/) | Free (250 contacts) | Email verification |
| **API Key** | Account ‚Üí Extras ‚Üí API keys | Free | Account owner |

**Mailchimp Key Locations:**
- **API Key:** Account ‚Üí Profile ‚Üí Extras ‚Üí API keys ‚Üí Create New Key
- **Server Prefix:** Found in API key (e.g., `us21` from `xxx-us21`)

#### Google Analytics
| Item | Sign-Up URL | Cost | Permissions Required |
|------|-------------|------|---------------------|
| **Google Account** | [accounts.google.com](https://accounts.google.com/signup) | Free | - |
| **GA4 Property** | [analytics.google.com](https://analytics.google.com) | Free | Google account |
| **Service Account** | [console.cloud.google.com](https://console.cloud.google.com/iam-admin/serviceaccounts) | Free | Editor role |
| **API Enable** | [console.cloud.google.com/apis](https://console.cloud.google.com/apis/library/analyticsdata.googleapis.com) | Free | Project owner |

**Google Analytics Key Locations:**
- **Property ID:** GA4 Admin ‚Üí Property ‚Üí Property details
- **Service Account JSON:** GCP Console ‚Üí IAM ‚Üí Service Accounts ‚Üí Keys ‚Üí Create key (JSON)
- **Measurement ID:** GA4 Admin ‚Üí Data Streams ‚Üí Web stream details

#### AI Models
- **Azure OpenAI Service:** (~$48-62/month estimated token usage - **REVISED**)
  - **Sign-Up:** [portal.azure.com](https://portal.azure.com) ‚Üí Create Resource ‚Üí Azure OpenAI
  - **Request Access:** [aka.ms/oai/access](https://aka.ms/oai/access) (if not already approved)
  - GPT-4o-mini for intent classification + Critic/Supervisor validation
  - GPT-4o for response generation
  - text-embedding-3-large for RAG embeddings
  - **Alternative:** Microsoft Foundry (Anthropic Claude via Azure) - also within secure perimeter
- **Tracing:** Azure Application Insights + Monitor Logs (~$10-15/month for execution traces)
  - **Sign-Up:** Azure Portal ‚Üí Create Resource ‚Üí Application Insights

**Budget Breakdown (Phase 4-5):**
- Azure infrastructure (Container Instances, Cosmos DB, Key Vault, Networking): ~$200-220/month
- AI models (OpenAI API calls, embeddings): ~$48-62/month
- Observability (Application Insights, Monitor Logs, execution tracing): ~$32-43/month
- Event bus (NATS JetStream via AGNTCY): ~$0 (reuses agent transport)
- Zendesk (if needed): ~$0-19/month (sandbox/trial)
- **Total:** ~$310-360/month

**Post Phase 5 Optimization Target:** $200-250/month (reduced agent count, optimized model selection, aggressive auto-scaling)

## GitHub Project Management

**Project Board**: https://github.com/orgs/Remaker-Digital/projects/1
**Repository**: https://github.com/Remaker-Digital/AGNTCY-muti-agent-deployment-customer-service

### Structure (as of 2026-01-22)
- **Epics**: 7 issues (#2-#8) - Actor-based organization
  - Customer Epic (#2) - 40 stories
  - Prospect Epic (#3) - 25 stories
  - Support Epic (#4) - 15 stories
  - Service Epic (#5) - 15 stories
  - Sales Epic (#6) - 15 stories
  - AI Assistant Epic (#7) - 5 stories
  - Operator Epic (#8) - 15 stories

- **User Stories**: 130 issues (#9-#138) organized by phase
  - Phase 1: 15 stories (#9-#23) ‚úÖ Complete
  - Phase 2: 50 stories (#24-#73) ‚è≥ Ready to start
  - Phase 3: 20 stories (#74-#93) - Testing
  - Phase 4: 30 stories (#94-#123) - Production deployment
  - Phase 5: 15 stories (#124-#138) - Go-live

- **Labels**: 30 total across 5 categories
  - Type: epic, feature, bug, enhancement, test, documentation
  - Priority: critical, high, medium, low
  - Component: infrastructure, agent, api, observability, testing, ci-cd, security, shared
  - Phase: phase-1, phase-2, phase-3, phase-4, phase-5
  - Actor: customer, prospect, support, service, sales, ai-assistant, operator

- **Milestones**: 5 phase-based milestones with due dates
  - Phase 1 - Infrastructure (2026-02-28) ‚úÖ Complete
  - Phase 2 - Business Logic (2026-04-30) ‚è≥ In progress
  - Phase 3 - Testing (2026-06-30)
  - Phase 4 - Production Deployment (2026-08-31)
  - Phase 5 - Go-Live (2026-09-30)

### Automation Scripts Created
Located in project root:
- `setup-github-cli.ps1` - GitHub CLI setup and authentication
- `create-labels.ps1` - Create all 30 project labels
- `create-epics-and-milestones.ps1` - Create 7 epics and 5 milestones
- `create-all-130-stories.ps1` - Create Phase 1 stories (15 issues)
- `create-remaining-115-stories.ps1` - Create Phase 2 stories (50 issues)
- `create-phases-3-4-5.ps1` - Create Phases 3-5 stories (65 issues)

**Total Success Rate**: 137/137 issues created (100%, 0 errors)

**See PROJECT-SETUP-COMPLETE.md for complete details**

## File Structure Guidelines

```
project-root/
‚îú‚îÄ‚îÄ agents/                      # Agent implementations (6 agents - REVISED 2026-01-22)
‚îÇ   ‚îú‚îÄ‚îÄ intent_classification/   # Each agent has own directory
‚îÇ   ‚îú‚îÄ‚îÄ knowledge_retrieval/
‚îÇ   ‚îú‚îÄ‚îÄ response_generation/
‚îÇ   ‚îú‚îÄ‚îÄ escalation/
‚îÇ   ‚îú‚îÄ‚îÄ analytics/
‚îÇ   ‚îî‚îÄ‚îÄ critic_supervisor/       # NEW: 6th agent for content validation
‚îú‚îÄ‚îÄ mocks/                       # Mock API implementations (Phase 1-3)
‚îÇ   ‚îú‚îÄ‚îÄ shopify/
‚îÇ   ‚îú‚îÄ‚îÄ zendesk/
‚îÇ   ‚îú‚îÄ‚îÄ mailchimp/
‚îÇ   ‚îî‚îÄ‚îÄ google_analytics/
‚îú‚îÄ‚îÄ shared/                      # Shared utilities
‚îÇ   ‚îú‚îÄ‚îÄ factory.py              # AGNTCY factory singleton
‚îÇ   ‚îú‚îÄ‚îÄ models.py               # Common message models
‚îÇ   ‚îî‚îÄ‚îÄ utils.py
‚îú‚îÄ‚îÄ tests/                       # Test suites
‚îÇ   ‚îú‚îÄ‚îÄ unit/                   # Phase 1
‚îÇ   ‚îú‚îÄ‚îÄ integration/            # Phase 2
‚îÇ   ‚îî‚îÄ‚îÄ e2e/                    # Phase 3
‚îú‚îÄ‚îÄ test-data/                   # Test fixtures (JSON)
‚îÇ   ‚îú‚îÄ‚îÄ shopify/
‚îÇ   ‚îú‚îÄ‚îÄ zendesk/
‚îÇ   ‚îú‚îÄ‚îÄ mailchimp/
‚îÇ   ‚îî‚îÄ‚îÄ conversations/
‚îú‚îÄ‚îÄ terraform/                   # Infrastructure-as-Code
‚îÇ   ‚îú‚îÄ‚îÄ phase1_dev/             # Local Docker equivalents
‚îÇ   ‚îî‚îÄ‚îÄ phase4_prod/            # Azure production
‚îú‚îÄ‚îÄ .github/
‚îÇ   ‚îî‚îÄ‚îÄ workflows/
‚îÇ       ‚îî‚îÄ‚îÄ dev-ci.yml          # GitHub Actions (Phase 1-3)
‚îú‚îÄ‚îÄ docs/                        # Documentation
‚îÇ   ‚îî‚îÄ‚îÄ dr-test-results/        # Disaster recovery test logs
‚îú‚îÄ‚îÄ docker-compose.yml           # Local development stack
‚îú‚îÄ‚îÄ azure-pipelines.yml          # Azure DevOps (Phase 4-5)
‚îú‚îÄ‚îÄ .env.example                 # Template (no secrets)
‚îú‚îÄ‚îÄ .gitignore                   # MUST ignore .env, secrets
‚îú‚îÄ‚îÄ requirements.txt             # Python dependencies
‚îú‚îÄ‚îÄ PROJECT-README.txt           # Main project specification
‚îú‚îÄ‚îÄ PROJECT-SETUP-COMPLETE.md    # GitHub project management setup summary
‚îú‚îÄ‚îÄ PROJECT-MANAGEMENT-BEST-PRACTICES-FOR-GITHUB.md  # PM best practices
‚îú‚îÄ‚îÄ PHASE-2-READINESS.md         # Phase 2 preparation and requirements
‚îú‚îÄ‚îÄ user-stories-phased.md       # Complete user story catalog
‚îú‚îÄ‚îÄ github-project-info.json     # Project metadata
‚îú‚îÄ‚îÄ AGNTCY-REVIEW.md            # SDK integration guide
‚îú‚îÄ‚îÄ CLAUDE.md                   # This file
‚îî‚îÄ‚îÄ README.md                   # Public-facing documentation
```

## Testing Guidance

### Unit Tests (Phase 1)
```python
# Example: Mock AGNTCY SDK components
from unittest.mock import Mock, patch
import pytest

@patch('agntcy_app_sdk.factory.AgntcyFactory')
def test_intent_classification(mock_factory):
    # Test agent logic without real SDK
    pass
```

### Integration Tests (Phase 2)
```python
# Example: Use mock API containers
import pytest
from testcontainers.compose import DockerCompose

@pytest.fixture(scope="session")
def docker_services():
    with DockerCompose(".", compose_file_name="docker-compose.test.yml") as compose:
        yield compose

def test_multi_agent_flow(docker_services):
    # Test against mock Shopify/Zendesk/Mailchimp
    pass
```

### Cost Monitoring (Phase 4-5)
```python
# Example: Cost validation in tests
def test_azure_resources_within_budget():
    # Use Azure Pricing API or Cost Management API
    # Assert monthly projected cost < $200
    pass
```

## Disaster Recovery Guidance

### RPO/RTO Targets
- **RPO:** 1 hour (max data loss acceptable)
- **RTO:** 4 hours (max recovery time)

### Backup Strategy (Phase 4-5)
- **Cosmos DB:** Continuous backup, point-in-time restore (30 days)
- **Terraform State:** Azure Blob Storage with versioning (30 day retention)
- **Container Images:** All production images retained indefinitely
- **Key Vault:** Soft-delete (90 days) + purge protection enabled

### DR Testing Schedule
- **Monthly:** Cosmos DB restore validation
- **Quarterly:** Full Terraform destroy/rebuild drill
- **Annually:** Tabletop exercise for all 5 DR scenarios

## Code Style and Conventions

### Python
- **Formatter:** black (line length 100)
- **Linter:** flake8
- **Type Hints:** Use for public APIs, optional for internal
- **Docstrings:** Google style, focus on "why" not "what"
- **Async:** Use async/await for AGNTCY SDK calls

### Comments
- Write for blog readers (educational audience)
- Explain architectural decisions and trade-offs
- Highlight cost optimization rationale
- Reference PROJECT-README.txt sections when relevant

### Naming
- Agents: `{purpose}_agent.py` (e.g., `intent_classification_agent.py`)
- Topics: `{agent-name}` (e.g., `intent-classifier`, `response-generator-en`)
- Containers: `{agent-name}:v{version}-{commit-sha}` (e.g., `intent-classifier:v1.0.0-abc123`)
- Environment vars: `UPPER_SNAKE_CASE` (e.g., `SLIM_ENDPOINT`, `COSMOS_CONNECTION_STRING`)

## Observability

### Phase 1-3 (Local)
- **Stack:** Grafana + ClickHouse + OpenTelemetry Collector (Docker Compose)
- **Endpoints:** Grafana (localhost:3001), OTLP (localhost:4318)
- **Tracing:** Enable via `AgntcyFactory(enable_tracing=True)`

### Phase 4-5 (Azure)
- **Stack:** Azure Monitor + Application Insights + Log Analytics
- **Retention:** 7 days (cost optimization)
- **Metrics:** Agent performance, response times, routing accuracy, costs
- **Alerts:** Latency >2min, error rate >5%, cost >80% budget

## Security Checklist

- [ ] All secrets in Azure Key Vault (Phase 4-5) or .env (Phase 1-3)
- [ ] .env files in .gitignore (never commit secrets)
- [ ] Managed identities for all Azure services (Phase 4-5)
- [ ] TLS 1.3 for all connections
- [ ] Network isolation for backend services (private endpoints)
- [ ] Pre-commit hooks: git-secrets, detect-secrets
- [ ] Dependency scanning: Dependabot, Snyk
- [ ] Secrets rotation procedures documented (quarterly)
- [ ] OWASP ZAP scanning in CI/CD pipeline

## Educational Considerations

This project is designed for a blog post audience. When writing code or documentation:

1. **Clarity over Brevity:** Prefer readable code with explanatory comments
2. **Cost Awareness:** Always explain cost implications of architectural choices
3. **Best Practices:** Demonstrate enterprise patterns (BCDR, IaC, CI/CD) even within budget
4. **Learning Path:** Code should be approachable for developers new to multi-agent systems
5. **Real-World Relevance:** Use realistic scenarios and data (e-commerce, customer service)
6. **Reproducibility:** All steps should be reproducible by blog readers on their own systems

## Common Questions & Answers

**Q: Why not use Azure Functions instead of Container Instances?**
A: Container Instances provide better isolation for long-running agents and align with AGNTCY SDK's design. Functions would require significant adaptation and may not support persistent agent sessions effectively.

**Q: Why Cosmos Serverless instead of provisioned throughput?**
A: Serverless mode provides pay-per-request billing, better for variable/unpredictable workloads, and easier to stay within $200/month budget. Provisioned RU/s has minimum costs even during idle periods.

**Q: Why not use managed Kubernetes (AKS)?**
A: AKS minimum costs (~$70-100/month for control plane + nodes) consume too much of the $200 budget. Container Instances provide adequate orchestration for 5 agents at much lower cost.

**Q: Can we use Azure OpenAI Service?**
A: Yes, but monitor token usage closely. Estimate $20-50/month for testing. Consider using canned responses in Phase 1-3 to avoid costs during development.

**Q: Why SLIM instead of NATS for transport?**
A: SLIM provides enterprise-grade security features out of the box. NATS is excellent for throughput but requires additional security configuration. For educational purposes, SLIM demonstrates secure patterns more clearly.

**Q: How do we handle the Zendesk cost if trial expires?**
A: Three options: (1) Use Zendesk Sandbox for partners/developers (free), (2) Budget $19-49/month and reduce Azure spend to $150-180, (3) Replace with free alternative like osTicket or FreshDesk free tier.

## When Stuck or Uncertain

1. **Check PROJECT-README.txt** for project requirements and constraints
2. **Review AGNTCY-REVIEW.md** for SDK integration patterns
3. **Verify budget impact** for any Azure service suggestion (Phase 4-5)
4. **Ask clarifying questions** about requirements before implementing
5. **Suggest alternatives** with cost/benefit trade-offs
6. **Document decisions** in code comments for blog readers
7. **Test locally first** before deploying to Azure (Phase 1-3 patterns apply to Phase 4-5)

## Success Criteria

The project is successful if:
- ‚úÖ Phase 1-3 runs entirely locally with $0 cloud costs
- ‚úÖ Phase 4-5 stays within $310-360/month Azure budget (REVISED 2026-01-22)
- ‚úÖ All 6 agents communicate via AGNTCY SDK successfully (REVISED: added Critic/Supervisor)
- ‚úÖ KPIs are measurable and demonstrable (even with mock data in Phase 1-3)
- ‚úÖ Code is educational, well-documented, and reproducible
- ‚úÖ Disaster recovery procedures are tested and validated
- ‚úÖ Security best practices are followed (secrets management, encryption, network isolation, PII tokenization)
- ‚úÖ CI/CD pipelines work reliably (GitHub Actions ‚Üí Azure DevOps)
- ‚úÖ Blog readers can follow along and build it themselves
- ‚úÖ Content validation (Critic/Supervisor) blocks malicious inputs and harmful outputs
- ‚úÖ Execution tracing provides full decision tree visibility for operators

## Final Notes

- **Priority Order:** Cost constraints > Functionality > Performance > Feature richness
- **Mindset:** This is a learning project - optimize for clarity and education
- **Budget:** $310-360/month is a HARD LIMIT for Phase 4-5 (REVISED 2026-01-22), with post-Phase 5 target of $200-250/month
- **Timeline:** No time estimates - focus on what needs to be done, not when
- **Audience:** Developers learning multi-agent AI, Azure, and cost optimization techniques

When in doubt, optimize for:
1. Educational value (blog readers learning)
2. Cost efficiency (demonstrate real-world budget constraints)
3. Reproducibility (readers can build it themselves)
4. Best practices (even within tight constraints)

---

## Current Status (2026-01-27)

| Metric | Value |
|--------|-------|
| **Phase** | Phase 5 In Progress |
| **Budget** | ~$214-285/month (within $310-360 limit) |
| **Agents** | 6 deployed (Intent, Knowledge, Response, Escalation, Analytics, Critic) |
| **Container IPs** | SLIM (10.0.1.4), NATS (10.0.1.5), Agents (10.0.1.6-11) |
| **Container Health** | **8/8 Running (0 restarts)** ‚úÖ |
| **Console** | localhost:8501 with Azure OpenAI ‚úÖ |
| **Code Optimization** | 42% agent code reduction (BaseAgent + formatters) |
| **Test Coverage** | 52% (116 passed, 15 failed, 23 skipped) |

**Azure Resources:** VNet, Cosmos DB, Key Vault, ACR, App Insights, 8 Container Groups
**Production Prompts:** 5 ready (intent, critic, escalation, response, judge)
**UCP Adoption:** Approved (80-120 hours, MCP binding)
**GitHub Project:** https://github.com/orgs/Remaker-Digital/projects/1

### Phase 5 Progress (2026-01-27)

**API Integrations - ALL COMPLETE:**
| Service | Status | Details |
|---------|--------|---------|
| Azure OpenAI | Configured | GPT-4o-mini, GPT-4o, embeddings |
| Mailchimp | Configured | "Remaker Digital" audience |
| Shopify | Configured | "Blanco" dev store (blanco-9939) |
| Zendesk | Configured | "Remaker Digital" trial (14 days) |
| Google Analytics | Configured | Property 454584057 |

**Load Testing Results:**
- 50 concurrent requests: 100% success, ~4 RPS per API
- Mailchimp rate limited at high concurrency (expected)

**Security Scan:**
- urllib3 CVE-2025-66418/66471/CVE-2026-21441: FIXED (upgraded to 2.6.3)
- protobuf CVE-2026-0994: Monitoring (no patch available)

**Azure Container Health - ALL FIXED ‚úÖ:**
| Container | IP | Status | Image |
|-----------|-----|--------|-------|
| SLIM Gateway | 10.0.1.4 | Running (0) | slim-gateway:latest (custom) |
| NATS | 10.0.1.5 | Running (0) | nats:2.10-alpine |
| Knowledge | 10.0.1.6 | Running (0) | knowledge-retrieval:v1.1.1-fix |
| Critic | 10.0.1.8 | Running (0) | critic-supervisor:v1.1.0-openai |
| Analytics | 10.0.1.9 | Running (0) | analytics:v1.1.0-openai |
| Intent | 10.0.1.9 | Running (0) | intent-classifier:v1.1.0-openai |
| Response | 10.0.1.10 | Running (0) | response-generator:v1.1.0-openai |
| Escalation | 10.0.1.11 | Running (0) | escalation:v1.1.0-openai |

**Container Fixes Applied (2026-01-27):**
1. **SLIM Gateway:** Created custom image with config baked in (`infrastructure/slim/Dockerfile`)
2. **Knowledge Retrieval:** Fixed Dockerfile imports, added `__init__.py`, updated `.dockerignore`

**Disaster Recovery:**
- Cosmos DB: Continuous backup, PITR from Jan 26
- Key Vault: Soft-delete + purge protection enabled

**Key Environment Variables:**
```bash
AZURE_OPENAI_ENDPOINT=https://your-resource.openai.azure.com/
AZURE_OPENAI_API_KEY=your-api-key
USE_AZURE_OPENAI=true
USE_REAL_APIS=true  # Enable real API integrations
```

> **Historical Updates:** See `docs/CLAUDE-HISTORY.md` for detailed session logs.

---

## Security Advisories

### Active: CVE-2026-0994 (protobuf)
- **Severity:** High | **Risk to Project:** Low
- **Status:** Monitoring - No patch available
- **Package:** protobuf 6.33.4 (transitive via streamlit)
- **Details:** `docs/SECURITY-ADVISORY-2026-01-27.md`
- **Next Review:** 2026-02-03

---

## Optimization & Maintenance

### Token Usage Optimization
This project includes tools to maintain efficient token usage during AI-assisted development:

**Automated Checks:**
```powershell
# Check for optimization opportunities
.\scripts\check-optimization.ps1

# Archive old CLAUDE.md content (dry run first)
.\scripts\archive-claude-history.ps1 -DryRun
.\scripts\archive-claude-history.ps1
```

**Key Files:**
- `docs/OPTIMIZATION-WORKFLOW.md` - Complete optimization procedures
- `scripts/check-optimization.ps1` - Automated status check
- `scripts/archive-claude-history.ps1` - Archive historical docs
- `shared/base_agent.py` - Shared agent boilerplate (341 lines)
- `requirements-agents.txt` - Consolidated agent dependencies

**Maintenance Schedule:**
| Task | Frequency | Trigger |
|------|-----------|---------|
| Archive CLAUDE.md | Weekly | >900 lines |
| Consolidate requirements | Monthly | New packages |
| Extract shared patterns | Quarterly | New agents |
| Split large files | As needed | >500 lines |

**Current Metrics (2026-01-27):**
- CLAUDE.md: 756 lines (144 lines until archive threshold)
- Total agent code: 2,337 lines (reduced 42% from ~4,000 lines)
- All 6 agents use BaseAgent pattern ‚úÖ
- Test coverage: 52% (116 passed, 15 failed, 23 skipped)

**Optimization Session 2026-01-27:**
| Agent | Before | After | Reduction |
|-------|--------|-------|-----------|
| Intent Classification | 626 | 434 | 31% |
| Knowledge Retrieval | 1,208 | 758 | 37% |
| Response Generation | 1,300 | 346 | 73% |
| Escalation | 633 | 517 | 18% |
| Analytics | 322 | 266 | 17% |
| Critic/Supervisor | 755 | 493 | 35% |

**Token Savings:** ~3,600-4,800 tokens/session from shared BaseAgent pattern
